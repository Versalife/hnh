<html>

<head>
    <link rel="stylesheet" href="./styles.css" type="text/css">
</head>

<body class="vsc-initialized">
    <h1>Sketches and Snippets</h1>
    <section>
        <section class="citation sketch">
            <div class="description">
                <h4>Interpreting Neural Networks using Graph Theory</h4>
                <h6><span class="conference cvpr">SKETCH</span> August 9, 2023 </h6>
                <!-- <h6><span class="conference cvpr">SKETCH</span></h6>
                <h5 datetime="2023-08-09">August 9, 2023</h5> -->
                <p>Neural Networks comprise of an ordered list of matrices with interspersed non-linear functions.
                    Every matrix is, in general, <a href="https://www.math3ma.com/blog/matrices-probability-graphs"
                        target="_blank">equivalent to a weighted bipartite graph</a>.
                    Therefore, a neural network can be viewed as an ordered list of weighted bipartite graphs.
                    Can we use this correspondence to translate the tools developed in Graph Theory to aid our
                    understanding of Neural Networks? What does Ford-Fulkerson have to say about how information flows
                    through a neural network?</p>
            </div>
        </section>

        <section class="citation">
            <div class="description">
                <h4>On Maths, Birds, and Frogs</h4>
                <h6><span class="conference workshop">SNIPPET</span> August13, 2023 </h6>
                <p>"... Rephrasing ideas in the precise language of mathematics allows
                    questions to be asked and answered in more useful, quantitative ways.
                    ... A bird's eye view of the landscape is a valuable perspective.
                    It allows the observer to discover unexpected connections between different parts.
                    However, in order to make those connections precise and rigorous, one requires the
                    frogs attention to detail. One need to, as the frog does, frolic in the mud.
                </p>
                <h5><a href="https://math3ma.institute/wp-content/uploads/2022/02/bradley_spring22.pdf"
                        target="_blank">Tai-Danae Bradley</a></h5>
            </div>
        </section>

        <section class="citation sketch">
            <div class="description">
                <h4>Proving Theorems is an Exercise in Breadth First Search</h4>
                <h6><span class="conference cvpr">SKETCH</span> August 11, 2023 </h6>
                <p>
                    Mathematical ideas (definitions, theorems ...), can be thought of as nodes in a directed
                    graph depicting the entailment relationships among them. When equipped with such an entailment
                    graph, proving a theorem becomes an exercise in searching for a path between some two particular
                    nodes.
                    To prove a theorem, we start by exploring nodes in the neighborhood of our proposition.
                    We systematically expand our search until we find the node corresponding to our conclusion. Since most 
                    theorems would include multiple nodes as starting points (i.e multiple premises), we could also think in terms
                    paths from meta-nodes to nodes,
                </p>
                <br>
                <p>
                    With this framework, we can note the following: First, different proofs to the same theorem
                    correspond to different paths between the same pair of nodes. Secondly, learning mathematics,
                    is the act of building a mental version of this entailment graph.
                </p>
                <br>
                <p>
                    This general idea can be extended beyond logical reasoning to encompass
                    <a href="https://en.wikipedia.org/wiki/Plausible_reasoning" target="_blank">plausible reasoning</a>
                    by equipping each edge with a non-negative weight representing the degree to which we believe that
                    the source entails the destination.
                </p>
            </div>
        </section>

        <section class="citation">
            <div class="description">
                <h4>de Tocqueville's Astute Observation </h4>
                <h6><span class="conference workshop">SNIPPET</span> August13, 2023 </h6>
                <p>
                    Read the language. Write the language. Speak the language.Understand the Language.
                    Why learn a language? Read literature written in that language. Interact with others who can
                </p>
                <h5><a href="https://en.wikipedia.org/wiki/Maryam_Mirzakhani" target="_blank">Maryam Mirzakhani</a></h5>
            </div>
        </section>


        <section class="citation sketch">
            <div class="description">
                <h4>On The Audible Rhyme</h4>
                <h6><span class="conference cvpr">SKETCH</span> August 22, 2023 </h6>
                <p>
                    There's a sense in which the process by which these Neural Networks are generated is similar to the
                    process
                    through which complex biological black boxes (otherwise known as organisms) are generated. More
                    specifically, there is an audible rhyme between backpropagation mediated model training via error
                    minimization and organismal evolution via natural selection. Evidently, both processes produce
                    systems that
                    demonstrate remarkable, useful traits. Further, both processes involve the
                    search through some abstract fitness landscape in order to find 'fit' models/organisms.
                </p>
                <br>
                <p>
                    Setting aside the philosophical question of whether deep learning models are like organisms (they
                    clearly aren't), what can we gain if we treat them as though they were? What, empirical, questions
                    can we ask of them? Within Biology, we have developed a rich set of experimental ideas, methods, and
                    tools in our quest to understand and interpret biological black boxes (i.e organisms and the cells
                    that comprise them). Is it possible to use these ideas, tools, and techniques to develop a general
                    framework for understanding and interpreting trained Neural Networks?
                </p>
                <br>
                <p>
                    Concretely, can we draw inspiration from Molecular Biology to design experiments geared towards
                    answering the following questions?
                    <li>Do the weights of neural networks, that is, the networks' learned representations, contain
                        functional modules? </li>
                    <li>Are these functional modules, if they exist, associated, in some fashion, to the traits
                        exhibited by the trained networks? </li>
                    <li>Can we isolate these functional modules, if they exist, and confirm their association to
                        particular traits through Gain or Loss of function experiments?</li>
                </p>
                <br>
                <div class="inline">
                    <h5><a href="https://colah.github.io/notes/bio-analogies/" target="_blank">Another Rough
                            Perspective</a></h5>
                    <h5><a href="https://github.com/jlikhuva/blog/blob/main/slides/Genetics+NN.pdf"
                            target="_blank">Preliminary Investigations</a></h5>
                </div>
            </div>
        </section>

        <section class="citation sketch">
            <div class="description">
                <h4>Find it; Lose it; Move it </h4>
                <h6><span class="conference cvpr">SKETCH</span> Summer 2022</h6>
                <p>
                    Read the language. Write the language. Speak the language.Understand the Language.
                    Why learn a language? Read literature written in that language. Interact with others who can
                </p>
                <h5><a href="https://vimeo.com/136532431"
                        target="_blank">Scott Gilbert</a></h5>
            </div>
        </section>

        <section class="citation sketch">
            <div class="description">
                <h4>What Happens If ... </h4>
                <h6><span class="conference cvpr">SKETCH</span> August 25, 2023 </h6>
                <!-- <h6><span class="conference cvpr">SKETCH</span></h6>
                <h5 datetime="2023-08-09">August 9, 2023</h5> -->
                <p>
                    ... we row reduce the matrices of a neural network after training? Intuitively, row reduction
                    is an iterative change of basis procedure. Therefore, the resultant matrices should be similar
                    to the original learned matrices. That is, they should represent the same underlying linear map,
                    just in a different basis. If we replace the matrices of the network with their
                    their row reduced counterparts, does the network retain its performance?
                    
                    Do we need to change the basis of the inputs to each layer in any fashion?
                </p>
                <br>
                <p>
                    In short, the question we are asking is this: "Are the traits/capabilities of a neural network
                    invariant under a change of basis?"
                    That is, are the traits basis free?
                </p>
            </div>
        </section>

        <section class="citation">
            <div class="description">
                <h4>Piketty's Powerful Opener</h4>
                <h6><span class="conference workshop">SNIPPET</span> Summer 2020</h6>
                <p>
                    "Every human society must justify its inequalities: unless reasons
                    for them are found, the whole political and social edifice stands in danger of collapse.
                    Every epoch, therefore, develops a range of contradictory discourses and ideologies for
                    the purpose of legitimizing the inequality that already exists or the people believe should exist.
                    ... In today's societies, these justificatory narratives comprise themes of property,
                    entrepreneurship,
                    and meritocracy ..."
                </p>

                <h5><a href="https://www.hup.harvard.edu/catalog.php?isbn=9780674980822" target="_blank">
                    Capital and ideology
                </a></h5>
            </div>
        </section>

        <section class="citation">
            <div class="description">
                <h4>Polya: On Plausible Reasoning in Mathematics</h4>
                <h6><span class="conference workshop">SNIPPET</span> Summer 2020</h6>
                <p>
                    "Mathematics is regarded as a demonstrative science.  Yet, this is onlyv one of its aspects. 
                    Finished mathematics presented in a finished form appears as purely demonstrative, 
                    consisting of proofs only. Yet, mathematics in the making resembles any other human knowledge in teh making.
                    You have to guess a mathematical theorem before you can prove it [that is, you have to come up with
                    a conjecture]. You have guess the idea of the proof before you carry through the details; You have to combine
                    observations and follow analogies; ... the result of the mathematician's creative work is demonstrative reasoning; 
                    but, the proof is discoverd by plausible reasoning, by guessing.
                    "
                </p>

                <h5><a href="https://press.princeton.edu/books/paperback/9780691025094/mathematics-and-plausible-reasoning-volume-1"
                    target="_blank">George Polya</a></h5>
            </div>
        </section>

        <section class="citation">
            <div class="description">
                <h4>Descartes + Rosen</h4>
                <h6><span class="conference workshop">SNIPPET</span> Summer 2020</h6>
                <p>
                    "... Just as proteins need to fold [into a 3D conformation] in order to become active, so to do
                    ideas.
                    [One] should fold ideas in order to bring them into close contact. [Great insights emerge from this
                    process]
                    "
                </p>
                <br>
                <p>
                    "... When we have intuitively understood some simple propositions ... it is useful to go through
                    them with a continuous, uninterrupted motion of thought, to meditate upon their mutual relations,
                    and to conceive,
                    distinctly several of them, as many as possible, simultaneously. in this manner, our knowledge will
                    grow more certain, and the capacity of the mind will notably increase
                    "
                </p>

                <h5><a href=http://dt.pepperdine.edu/descartes-rules-for-direction-of-the-mind.html target="_blank">
                        Rules for the Direction of the Mind
                    </a></h5>
            </div>
        </section>


    </section>
</body>

</html>