[["Map",1,2,9,10,169,170],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.3","content-config-digest","e4236c858a5454c3","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://versalife.github.io\",\"compressHTML\":true,\"base\":\"/hnh\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{\"rs\":\"rust\"},\"theme\":\"github-light\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[null],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","writing",["Map",11,12,22,23,32,33,41,42,50,51,67,68,80,81,89,90,98,99,107,108,116,117,130,131,139,140,148,149,157,158],"analysis-tools",{"id":11,"data":13,"body":18,"filePath":19,"digest":20,"legacyId":21,"deferredRender":17},{"title":14,"date":15,"tags":16,"draft":17},"Analysis Tools in rand-NLA",["Date","2025-06-01T00:00:00.000Z"],[],true,"Coming soon.","src/content/writing/analysis-tools.mdx","6b692ef97cafa812","analysis-tools.mdx","gene",{"id":22,"data":24,"body":28,"filePath":29,"digest":30,"legacyId":31,"deferredRender":17},{"title":25,"date":26,"tags":27,"draft":17},"The Genetics of Neural Networks: A Lab Notebook",["Date","2023-08-01T00:00:00.000Z"],[],"## Introduction\n\n## Background\n\n## Methodology\n\n## Results\n\n## Discussion\n\n## References","src/content/writing/gene.mdx","98255f95e1db2334","gene.mdx","orth",{"id":32,"data":34,"body":18,"filePath":38,"digest":39,"legacyId":40,"deferredRender":17},{"title":35,"date":36,"tags":37,"draft":17},"Orthogonalization, Gram-Schmidt, and Statistical Independence",["Date","2025-08-01T00:00:00.000Z"],[],"src/content/writing/orth.mdx","16f8995b280e2ca6","orth.mdx","mtx-graph",{"id":41,"data":43,"body":18,"filePath":47,"digest":48,"legacyId":49,"deferredRender":17},{"title":44,"date":45,"tags":46,"draft":17},"Treating a Matrix as a Graph",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/writing/mtx-graph.mdx","2fcf81faebac74a7","mtx-graph.mdx","rmq",{"id":50,"data":52,"body":63,"filePath":64,"digest":65,"legacyId":66,"deferredRender":17},{"title":53,"date":54,"description":55,"tags":56,"draft":60,"citationKey":61,"citationUrl":62},"Rusty Solutions to the RMQ Problem",["Date","2023-08-01T00:00:00.000Z"],"A progression of range minimum query solutions in Rust: from O(n²) dense tables to O(n) Fischer-Heun via sparse tables, block decomposition, and Cartesian trees.",[57,58,59],"algorithms","data-structures","rust",false,"jlikhuva2021rmq","https://github.com/jlikhuva/blog/blob/main/posts/rmq.md","## The Problem\n\nGiven a static array $A$ containing comparable elements, design data structures and algorithms such that given an interval $(i, j)$, we can retrieve the index of the minimum element within the subarray $A[i..j]$. We need to minimize the preprocessing time, the memory usage, and the time it takes to answer any query.\n\n## A Naïve Solution\n\nThe most straightforward way to solve this problem is to create a lookup table with all the RMQ answers precomputed. This will allow us to answer any RMQ in constant time by doing a table lookup. How can we build such a table? The first thing to notice is that this is a discrete optimization problem — we are interested in the minimal (aka the optimal) value in a given range. This means that we may be able to use dynamic programming to solve the problem. All we need to do is come up with an update rule.\n\nSuppose our array is $A$, if we know the smallest value in some range $(i, j)$ to be $\\alpha$, we can easily figure out the answer on a larger range $(i, j+1)$ by comparing $\\alpha$ with $A[j + 1]$. That is:\n\n$$\n\\text{rmq}_A(i, j) = \\begin{cases}\nA[j], & \\text{if}\\ i = j \\\\\n\\min\\left(A[j], \\text{rmq}_A(i, j - 1)\\right), & \\text{otherwise}\n\\end{cases}\n$$\n\nWe can do this for all possible values of $i$ and $j$ to fill up our lookup table. This takes quadratic time. Thus, with this approach, we can solve the RMQ problem in $\\langle\\Theta(n^2), \\Theta(1)\\rangle$.\n\nThe code below implements this approach. The only modification we make is that instead of calculating the actual minimal value, we calculate the index of the smallest value. That is $\\arg\\min$ instead of $\\min$.\n\n```rust\n/// An inclusive ([i, j]), 0 indexed range for specifying a range\n/// query.\n#[derive(Eq, PartialEq, Hash)]\npub struct RMQRange\u003C'a, T> {\n    /// The starting index, i\n    start_idx: usize,\n\n    /// The last index, j\n    end_idx: usize,\n\n    /// The array to which the indexes above refer. Keeping\n    /// a reference here ensures that some key invariants are\n    /// not violated. Since it is expected that the underlying\n    /// array will be static, we'll never make a mutable reference\n    /// to it. As such, storing shared references in many\n    /// different RMQRange objects should be fine\n    underlying: &'a [T]\n}\n```\n\nTo make it easy to construct new range objects, we implement the from trait. This implementation will allow us to construct an `RMQRange` object from a `3-Tuple` by simply invoking `(a, b, c).into`. We also do error checking here to make sure that we can only ever create valid ranges.\n\n```rust\nimpl \u003C'a, T> From\u003C(usize, usize, &'a [T])> for RMQRange\u003C'a, T> {\n    fn from(block: (usize, usize, &'a [T])) -> Self {\n        let start_idx = block.0;\n        let end_idx = block.1;\n        let len = block.2.len();\n        if start_idx > end_idx {\n            panic!(\"Range start cannot be larger than the range's end\")\n        }\n        if end_idx >= len {\n            panic!(\"Range end cannot be >= the len of underlying array\")\n        }\n        RMQRange {\n            start_idx,\n            end_idx,\n            underlying: block.2\n        }\n    }\n}\n```\n\nWith the abstractions above, we can go ahead and implement our procedure.\n\n```rust\nuse std::collections::HashMap;\nuse std::hash::Hash;\n\ntype LookupTable\u003C'a, T> = HashMap\u003CRMQRange\u003C'a, T>, usize>;\n\n/// Computes the answers to all possible queries. Since the ending index of a query\n/// is lower bounded by the starting index, the resulting lookup table is an\n/// upper triangular matrix. Therefore, instead of representing it as a matrix,\n/// we use a hashmap instead (to save space)\nfn compute_rmq_all_ranges\u003CT: Hash + Eq + Ord>(array: &[T]) -> LookupTable\u003C'_, T> {\n    let len = array.len();\n    let capacity = if len > 0 { len * (len + 1) / 2 } else { 0 };\n    let mut lookup_table = HashMap::with_capacity(capacity);\n    for start in 0..len {\n        let mut current_min_idx = start;\n        for end in start..len {\n            if array[end] \u003C array[current_min_idx] {\n                current_min_idx = end;\n            }\n            let range: RMQRange\u003C'_, T> = (start, end, array).into();\n            lookup_table.insert(range, current_min_idx);\n        }\n    }\n    lookup_table\n}\n```\n\nYou can play around with the code so far [in the playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=ccb49819827b6e1834765389f7ecf12b).\n\nCan we do better than $\\langle\\Theta(n^2), \\Theta(1)\\rangle$? The query time is the best we can ever hope for. However, we can reduce the processing time. Let's see how we can do that in the next section.\n\n---\n\n## Binary Representation & Sparse Tables\n\nAny positive integer can be factored into a sum of powers of two. This binary factorization is the basis of binary representation. For instance, the decimal number 19 can be represented as:\n\n$$19 = 16 + 2 + 1 = 2^4 + 0 \\cdot 2^3 + 0 \\cdot 2^2 + 2^1 + 2^0 = (10011)_2$$\n\nGiven some range $[i, j]$ we know that its length, $j - i + 1$ is positive. We can therefore factor it using binary factorization to get shorter ranges. For instance, if our range is $(0, 18)$ we see that it has a length of $19$ which, as we saw above, can be factored into $(0, 15) + (16, 17) + (18, 18)$.\n\n### Preprocessing\n\nHow can we use these observations to construct a solution to our problem? First, note that powers of two are sparsely distributed among positive integers. Also, because they can be combined to form any other number, if we had a table with answers to all possible ranges whose size is a power of two, we would be able to get answers for any range. How can we construct such sparse table?\n\nFor an array of length $k$, there are $\\log k$ ranges whose size is a power of two (just as there are $\\lg x$ bits in the binary representation of $x$). We shall thus construct the sparse table by computing answers to all $\\lg k$ ranges for all $n$ possible starting positions. Therefore, the time needed to create the sparse table is $\\mathcal{O}(n \\log n)$. We implement this scheme below.\n\n```rust\nuse std::collections::HashMap;\nuse std::hash::Hash;\n\n/// An index into our sparse table\n#[derive(Debug, Eq, PartialEq, Hash, Clone)]\npub struct SparseTableIdx {\n    /// The index where the range in question begins\n    start_idx: usize,\n\n    /// The length of the range. This has to always be a power of\n    /// two\n    len: usize\n}\n\nimpl From\u003C(usize, usize)> for SparseTableIdx {\n    fn from(idx_tuple: (usize, usize)) -> Self {\n        let start_idx = idx_tuple.0;\n        let len = idx_tuple.1;\n        if len == 0 || !len.is_power_of_two() {\n            panic!(\"Expected the length to be a non-zero power of 2, got {}\", len)\n        }\n        SparseTableIdx { start_idx, len }\n    }\n}\n```\n\n```rust\n/// A sparse table is simply a collection of rmq answers for ranges whose\n/// length is a power of two. We pre-compute such ranges for all possible starting\n/// positions\ntype SparseTable\u003C'a, T> = HashMap\u003CSparseTableIdx, RMQResult\u003C'a, T>>;\n\n/// For each index `i`, compute RMQ answers for ranges starting at `i` of\n/// size `1, 2, 4, 8, 16, …, 2^k` as long as the resultant ending index\n/// fits in the underlying array in the array.\n/// For each array index, we compute lg n ranges. Therefore,\n/// the total cost of the procedure is O(n lg n)\nfn compute_rmq_sparse\u003C'a, T: Ord + Hash + Eq>(array: &'a [T]) -> SparseTable\u003C'a, T> {\n    let len = array.len();\n    if len == 0 {\n        return HashMap::new();\n    }\n    let capacity_estimate = if len > 1 { len * (usize::BITS - len.leading_zeros()) as usize } else { len };\n    let mut sparse_table = HashMap::with_capacity(capacity_estimate);\n\n    // Base case: ranges of length 1 (2^0)\n    for i in 0..len {\n        let idx: SparseTableIdx = (i, 1).into();\n        let rmq_res: RMQResult\u003C'a, T> = (i, &array[i]).into();\n        sparse_table.insert(idx, rmq_res);\n    }\n\n    // Build for ranges of length 2^power\n    let mut power = 1;\n    while (1 \u003C\u003C power) \u003C= len {\n        let current_len = 1 \u003C\u003C power;\n        let prev_len = 1 \u003C\u003C (power - 1);\n\n        for start_idx in 0..=(len - current_len) {\n            let idx: SparseTableIdx = (start_idx, current_len).into();\n            let left_idx: SparseTableIdx = (start_idx, prev_len).into();\n            let right_idx: SparseTableIdx = (start_idx + prev_len, prev_len).into();\n\n            let left_res = sparse_table.get(&left_idx).expect(\"Previous smaller range must exist\");\n            let right_res = sparse_table.get(&right_idx).expect(\"Previous smaller range must exist\");\n\n            let rmq_res = get_prev_min(array, left_res, right_res);\n            sparse_table.insert(idx, rmq_res);\n        }\n        power += 1;\n    }\n    sparse_table\n}\n\nfn get_prev_min\u003C'a, T: Ord>(\n    _array: &'a [T],\n    left_res: &RMQResult\u003C'a, T>,\n    right_res: &RMQResult\u003C'a, T>,\n) -> RMQResult\u003C'a, T> {\n    if left_res.min_value \u003C= right_res.min_value {\n        RMQResult { min_idx: left_res.min_idx, min_value: left_res.min_value }\n    } else {\n        RMQResult { min_idx: right_res.min_idx, min_value: right_res.min_value }\n    }\n}\n```\n\n### Querying the Sparse Table\n\nNow that we have our sparse table, how can we query from it given an arbitrary range $R = [i, j]$? From our initial discussion of binary factorization, you can imagine computing all sub-ranges of $R$ whose length is a power of 2 and then taking the min over these values. For an arbitrary length $n$, there are $O(\\lg n)$ such sub-ranges. Thus, this scheme would give us a $\\langle\\Theta(n \\lg n), \\Theta(\\lg n)\\rangle$ solution to the `RMQ` problem.\n\nComputing all sub-ranges, however, is overkill. All we need are two sub-ranges that fully cover the underlying segment. How do we find the two covering segments? First, observe that if the length of the range is an exact power of two, then we do not need to do any further computation since we already precomputed answers for all such ranges. If its not, we start by finding the largest sub-range that is an exact power of two. Specifically, we find the value $k$ such that $2^k \\le (j - i) + 1$. Note that this value $k$ is the index of the most significant bit of the range's length (or more precisely, $\\lfloor \\log_2(j - i + 1) \\rfloor$). The first range is thus $[i, i + 2^k - 1]$. That is, a range of length $2^k$. After finding the largest sub-range whose length is a power of two starting at $i$, the remaining portion might not be fully covered. To proceed, we use a neat trick: we take another range of the *same* length $2^k$, but this one *ends* at $j$. This ensures coverage. The second range is thus $[j - 2^k + 1, j]$, also with a length of $2^k$. These two ranges might overlap, but together they cover the original range $[i, j]$.\n\nTo recapitulate, we query from the sparse table by finding the $\\arg\\min$ of two overlapping ranges (both of length $2^k$) whose answers have already been computed. Figuring out which ranges to use involves finding $k = \\lfloor \\log_2(n) \\rfloor$ where $n$ is the length of the range in the query. How do we calculate $k$? To compute $k$ in constant time, we can use a lookup table or built-in functions for finding the most significant bit (MSB) or calculating the integer logarithm. Later on, when discussing specialized integer containers, we might implement a complex but straightforward method for finding $k$ in constant time. For now, a lookup table suffices. Thus, with this scheme, we have a $\\langle\\Theta(n \\lg n), \\Theta(1)\\rangle$ solution to the `RMQ` problem. Below, we implement a procedure to compute the lookup table.\n\n```rust\nuse std::cmp;\n\nconst LOOKUP_TABLE_SIZE: usize = 1 \u003C\u003C 16;\n\n/// Lookup table for the index of the most significant bit (MSB).\n/// Stores floor(log2(n)) for n from 1 to LOOKUP_TABLE_SIZE.\npub struct MSBLookupTable([u8; LOOKUP_TABLE_SIZE]);\n\nimpl MSBLookupTable {\n    /// Build the lookup table.\n    pub fn build() -> Self {\n        let mut lookup_table = [0u8; LOOKUP_TABLE_SIZE];\n        for i in 2..=LOOKUP_TABLE_SIZE {\n            lookup_table[i - 1] = lookup_table[(i / 2) - 1] + 1;\n        }\n        MSBLookupTable(lookup_table)\n    }\n\n    /// Get the index of the most significant bit (floor(log2(n))) for a usize value n.\n    /// Assumes n > 0.\n    pub fn get_msb_idx_of(&self, n: usize) -> u8 {\n        debug_assert!(n > 0, \"get_msb_idx_of called with n=0\");\n\n        const BITS: u32 = usize::BITS;\n\n        if BITS == 64 {\n            if n >> 48 != 0 {\n                self.0[((n >> 48) as u16) - 1] + 48\n            } else if n >> 32 != 0 {\n                self.0[((n >> 32) as u16) - 1] + 32\n            } else if n >> 16 != 0 {\n                self.0[((n >> 16) as u16) - 1] + 16\n            } else {\n                self.0[(n as u16) - 1]\n            }\n        } else {\n            if n >> 16 != 0 {\n                self.0[((n >> 16) as u16) - 1] + 16\n            } else {\n                self.0[(n as u16) - 1]\n            }\n        }\n    }\n}\n```\n\nOnce again, the query time is the best possible. However, even though the pre-processing time reduced from quadratic to $O(n \\log n)$, we can still do better. In particular, we can shave off a log factor and arrive at a linear time pre-processing algorithm. To figure out how to do that, we shall take a detour to discuss the method of four russians.\n\nIf you'd like to take a breather, feel free to play around with the sparse table code [in the rust playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=7f9c152dee95816d7ef8ef9d14bc1f72).\n\n---\n\n### The Method of Four Russians\n\nWe begin this detour by taking another detour. Let us discuss the algorithms used to find the median (or more generally, the $i^{th}$ order statistic) of a collection of pairwise comparable items. `Quickselect` can solve this problem in expected linear time. However, if we want a worst case linear time solution, we need to use the *Median of Medians* procedure.\n\n`MoM` is exactly similar `Quickselect` except, instead of randomly picking the index to partition around, we compute an approximate median value. We begin by dividing the input collection into blocks of length=5. This gives us $\\lceil n/5 \\rceil$ blocks, with the final block possibly having $\u003C 5$ items. For each block, we calculate the median by first sorting and selecting the lower median. For a single block, this always takes constant time, meaning that finding the median for all blocks takes linear time. We aggregate all the block-level medians into a single array. This array is of length $\\lceil n/5 \\rceil$. Once we have aggregated the block level medians, we are faced with the exact same problem we started with — just on a much smaller array. Therefore, we can recursively find the median of this new array. Once we have this value, we can proceed as usual, using the [prune and conquer](https://www.notion.so/A-note-on-algorithmic-design-patterns-20e50d39c99945e3ad8dfb804177ab3f) strategy. Below, we implement this scheme.\n\n```rust\n/// The abstraction for a single block (conceptual).\n#[derive(Debug)]\npub struct MedianBlock\u003C'a, T> {\n    /// The starting index in the original array.\n    start_idx: usize,\n    /// The ending index (inclusive) in the original array.\n    end_idx: usize,\n    /// The index of the median *within the original array*.\n    median_original_idx: usize,\n    /// Reference to the median value.\n    median_value: &'a T,\n}\n\nfn find_median_of_block\u003C'a, T: Ord + Clone>(block_slice: &'a [T], start_idx: usize) -> (usize, &'a T) {\n    if block_slice.is_empty() {\n        panic!(\"Cannot find median of empty block\");\n    }\n    let mut indexed_block: Vec\u003C(usize, &T)> = block_slice.iter().enumerate().collect();\n    indexed_block.sort_unstable_by_key(|&(_, val)| val);\n    let median_local_idx = indexed_block.len() / 2;\n    let (original_local_idx, median_val) = indexed_block[median_local_idx];\n    (start_idx + original_local_idx, median_val)\n}\n\nimpl\u003C'a, T: Ord + Clone> MedianBlock\u003C'a, T> {\n    fn new(start: usize, end: usize, median_idx: usize, median_val: &'a T) -> Self {\n        MedianBlock {\n            start_idx: start,\n            end_idx: end,\n            median_original_idx: median_idx,\n            median_value: median_val,\n        }\n    }\n}\n```\n\nWith the above abstractions in place, we can go ahead and implement the main procedure.\n\n```rust\nuse std::cmp::Ordering;\n\nfn partition_around_pivot\u003CT: Ord>(array: &mut [T], pivot_idx: usize) -> usize {\n    let pivot_final_idx = pivot_idx; // Placeholder\n    pivot_final_idx\n}\n\n/// Computes the k-th smallest element (0-indexed) in the `array`.\n/// This is sometimes referred to as the k-th order statistic. O(n) worst-case.\nfn kth_order_statistic\u003C'a, T: Ord + Clone>(array: &'a mut [T], k: usize) -> &'a T {\n    let n = array.len();\n    if k >= n {\n        panic!(\"k must be less than array length\");\n    }\n\n    if n \u003C= 5 {\n        array.sort_unstable();\n        return &array[k];\n    }\n\n    // 1. Divide into blocks of 5\n    let num_blocks = (n + 4) / 5;\n    let mut medians = Vec::with_capacity(num_blocks);\n    for i in 0..num_blocks {\n        let start = i * 5;\n        let end = std::cmp::min(start + 5, n);\n        let block_slice = &array[start..end];\n        let (_median_original_idx, median_val_ref) = find_median_of_block(block_slice, start);\n        medians.push(median_val_ref.clone());\n    }\n\n    // 2. Find median of medians recursively\n    let median_of_medians_val = kth_order_statistic(&mut medians, num_blocks / 2).clone();\n\n    // 3. Find the original index of the median of medians\n    let pivot_original_idx = array.iter().position(|x| *x == median_of_medians_val)\n                                .expect(\"Median of medians must be in the original array\");\n\n    // 4. Partition around the approximate pivot\n    let pivot_final_idx = partition_around_pivot(array, pivot_original_idx);\n\n    // 5. Recurse into the appropriate partition\n    match k.cmp(&pivot_final_idx) {\n        Ordering::Equal => &array[pivot_final_idx],\n        Ordering::Less => kth_order_statistic(&mut array[..pivot_final_idx], k),\n        Ordering::Greater => kth_order_statistic(&mut array[pivot_final_idx + 1..], k - (pivot_final_idx + 1)),\n    }\n}\n```\n\nYou can play around with the code for computing the `kth_order_statistic` [in the playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=02f915a79be3e7b6aadf53cfc1f29156).\n\n---\n\nThe median of medians procedure has a few key structures:\n\n- The input is divided into blocks of equal size. This is called block partitioning and each block is called the micro array.\n- The original problem (median in this case) is solved for each block using a naive method that works well for small input sizes. With this scheme, we are able to solve the problem for each block in constant time and for all blocks in linear time.\n- The solutions to all blocks are aggregated into a single array. We call this the macro array. The macro array, just like the micro arrays, are smaller instances of the original problem.\n- By combining, in some bespoke fashion, the macro and micro array solutions, we are able to solve the original problem with a log factor shaved off. In `MoM` we went from `Quickselect's` $O(n \\log n)$ to $O(n)$ (For a rigorous runtime analysis of the median of medians method, please refer to CLRS chapter 9).\n\nThe structures above are the four major motifs in the method of four russians. How can we use this method to reduce the pre-processing time of our RMQ algorithm? We discuss that after the following interlude.\n\n---\n\nThus far, we've implemented procedures to solve the `rmq` problem as free standing functions. Before we move forward, let's take a step back and see if we can come up with a much more elegant abstraction that unifies all the different solution methods. This will become more crucial as we start talking about 2-level structures that use multiple solution methods.\n\n```rust\nuse std::collections::HashMap;\nuse std::hash::Hash;\nuse std::cmp::Ord;\n\n#[derive(Debug, Clone)]\npub struct RMQResult\u003C'a, T> {\n    pub min_idx: usize,\n    pub min_value: &'a T,\n}\n\nimpl\u003C'a, T> From\u003C(usize, &'a T)> for RMQResult\u003C'a, T> {\n    fn from((min_idx, min_value): (usize, &'a T)) -> Self {\n        RMQResult { min_idx, min_value }\n    }\n}\n\ntype DenseTable\u003C'a, T> = HashMap\u003CRMQRange\u003C'a, T>, RMQResult\u003C'a, T>>;\ntype SparseTableMap\u003C'a, T> = HashMap\u003CSparseTableIdx, RMQResult\u003C'a, T>>;\n\n/// All structures capable of answering range min queries should\n/// expose the solve method.\npub trait RMQSolver\u003C'a, T: Ord> {\n    fn solve(&self, range: &RMQRange\u003C'a, T>) -> RMQResult\u003C'a, T>;\n}\n```\n\nWe introduce a trait that encodes the necessary and sufficient API that any `rmq` solver should expose. We need to be able to build the solver and to invoke the solve method with a given range. Below, we introduce the various solvers, all of which we have already seen before — we simply present them here in a unified manner.\n\n```rust\n/// A solver that answers range min queries by doing no preprocessing. At query time, it\n/// simply does a linear scan of the range in question to get the answer. This is an\n/// \u003CO(1), O(n)> solver.\n#[derive(Debug)]\npub struct ScanningSolver\u003C'a, T> {\n    underlying: &'a [T],\n}\n\nimpl\u003C'a, T> ScanningSolver\u003C'a, T> {\n    pub fn new(underlying: &'a [T]) -> Self {\n        ScanningSolver { underlying }\n    }\n}\n\n/// A solver that answers `rmq` queries by first pre-computing\n/// the answers to all possible ranges. At query time, it simply\n/// makes a table lookup. This is the \u003CO(n^2), O(1)> solver.\n#[derive(Debug)]\npub struct DenseTableSolver\u003C'a, T: Ord + Hash + Eq> {\n    underlying: &'a [T],\n    lookup_table: DenseTable\u003C'a, T>,\n}\n\nimpl\u003C'a, T: Ord + Hash + Eq> DenseTableSolver\u003C'a, T> {\n    pub fn new(underlying: &'a [T]) -> Self {\n        let lookup_table = compute_rmq_all_ranges_results(underlying);\n        DenseTableSolver {\n            underlying,\n            lookup_table,\n        }\n    }\n}\n\n/// A solver that answers rmq queries by first precomputing\n/// the answers to ranges whose length is a power of 2.\n/// At query time, it uses a lookup table of `msb(n)` values to\n/// combine answers from the sparse table.\n/// This is the \u003CO(n log n), O(1)> solver.\n#[derive(Debug)]\npub struct SparseTableSolver\u003C'a, T: Ord + Hash + Eq> {\n    underlying: &'a [T],\n    sparse_table: SparseTableMap\u003C'a, T>,\n    msb_lookup: MSBLookupTable,\n}\n\nimpl\u003C'a, T: Ord + Hash + Eq> SparseTableSolver\u003C'a, T> {\n    pub fn new(underlying: &'a [T]) -> Self {\n        let sparse_table = compute_rmq_sparse(underlying);\n        let msb_lookup = MSBLookupTable::build();\n        SparseTableSolver {\n            underlying,\n            sparse_table,\n            msb_lookup,\n        }\n    }\n}\n```\n\nBelow, we implement the `RMQSolver` trait for each of our solvers. We leverage functions that we already implemented in preceding segments.\n\n```rust\nuse std::cmp::Ordering;\n\nfn get_min_by_scanning\u003C'a, T: Ord>(block_slice: &'a [T], original_start_idx: usize) -> RMQResult\u003C'a, T> {\n    if block_slice.is_empty() {\n        panic!(\"Cannot scan empty slice\");\n    }\n    let (min_local_idx, min_value_ref) = block_slice\n        .iter()\n        .enumerate()\n        .min_by_key(|&(_, val)| val)\n        .unwrap();\n\n    (original_start_idx + min_local_idx, min_value_ref).into()\n}\n\n\nimpl\u003C'a, T: Ord> RMQSolver\u003C'a, T> for ScanningSolver\u003C'a, T> {\n    fn solve(&self, range: &RMQRange\u003C'a, T>) -> RMQResult\u003C'a, T> {\n        if range.start_idx > range.end_idx || range.end_idx >= self.underlying.len() {\n            panic!(\"Invalid range for solve: {:?}\", range);\n        }\n        let range_slice = &self.underlying[range.start_idx..=range.end_idx];\n        get_min_by_scanning(range_slice, range.start_idx)\n    }\n}\n\nimpl\u003C'a, T: Ord + Eq + Hash> RMQSolver\u003C'a, T> for DenseTableSolver\u003C'a, T> {\n    fn solve(&self, range: &RMQRange\u003C'a, T>) -> RMQResult\u003C'a, T> {\n        let result_ref = self.lookup_table.get(range)\n                                .expect(\"Range not found in dense table.\");\n        result_ref.clone()\n    }\n}\n\nimpl\u003C'a, T: Ord + Eq + Hash> RMQSolver\u003C'a, T> for SparseTableSolver\u003C'a, T> {\n    fn solve(&self, range: &RMQRange\u003C'a, T>) -> RMQResult\u003C'a, T> {\n        let i = range.start_idx;\n        let j = range.end_idx;\n        if i > j { panic!(\"Invalid range i > j\"); }\n        let range_len = (j - i) + 1;\n\n        if range_len == 0 {\n            panic!(\"Cannot solve for empty range\");\n        }\n\n        let k = self.msb_lookup.get_msb_idx_of(range_len);\n        let block_len = 1 \u003C\u003C k;\n\n        let left_idx: SparseTableIdx = (i, block_len).into();\n        let right_start = j - block_len + 1;\n        let right_idx: SparseTableIdx = (right_start, block_len).into();\n\n        let left_res = self.sparse_table.get(&left_idx)\n                                .expect(\"Left block index not found in sparse table\");\n        let right_res = self.sparse_table.get(&right_idx)\n                                .expect(\"Right block index not found in sparse table\");\n\n        get_prev_min(self.underlying, left_res, right_res)\n    }\n}\n```\n\n---\n\n### Two-Level Structures\n\nTo apply the method of four russians to the RMQ problem, we begin by dividing the input array into blocks of length $b$. If the length of the array is $n$, this results in $O(n/b)$ blocks. For each of these blocks, we find the index of the smallest value by doing a simple scan. This takes $O(b)$ in each block and $O(n/b) \\cdot O(b) = O(n)$ for all the blocks. We aggregate these min values (or their indices) in a new macro array. Given a query range $[i, j]$ how can we use the blocks and the macro array to satisfy the query? Also, what value of $b$ should we use?\n\nTo query, we start by figuring out which block the ends of the query fall into. We do that by dividing each end with the block size, i.e `start_block = i/b`, `end_block = j/b`. We then scan the items in `start_block` that appear at or after index $i$ and the items in `end_block` that appear at or before index $j$ and take the minimal value over them. Let's call this value, the smallest value at the ends of the range, $\\lambda$. Then we query the macro array to find the minimal value among all blocks strictly between `start_block` and `end_block` (i.e., from `start_block + 1` to `end_block - 1`). Let's call this value $\\alpha$. The answer to our query is the $\\min$ (or $\\arg\\min$) between these two values (and potentially values from the partial start/end blocks): $RMQ_A(i, j) = \\min(\\lambda, \\alpha)$. How long does this take? Finding the $\\min$ in the potentially partial end blocks takes $O(b)$ by scanning. Querying the intermediate blocks in the macro array depends on the method used for the macro array. If we scan the macro array as well, it takes $O(n/b)$. This gives a total query time of $O(b + n/b)$. Therefore, to properly characterize the runtime, we need to find the value of $b$ that minimized the expression $b + n/b$. We do so below:\n\n$$f(b) = b + \\frac{n}{b}, \\quad f'(b) = 1 - \\frac{n}{b^2} = 0 \\implies b^2 = n \\implies b = \\sqrt{n}$$\n\nSo, we set $b$ to the square root of $n$. This gives us a query time of $O(\\sqrt{n})$ and an overall time complexity (preprocessing, query) of $\\langle O(n), O(n^{0.5}) \\rangle$.\n\nSince our two level structure solutions will eventually mix and match the solvers that they use at each level, we begin by introducing an abstraction to facilitate that. Below, we implement an object that can answer any range min query using parameters that can be set by the client.\n\n```rust\nuse std::collections::HashMap;\nuse std::hash::Hash;\nuse std::cmp::Ord;\nuse std::marker::PhantomData;\n\n/// The abstraction for storing block-level minimum info.\n#[derive(Debug, Clone)]\npub struct RMQBlockInfo\u003C'a, T: Ord> {\n    pub start_idx: usize,\n    pub end_idx: usize,\n    pub min_original_idx: usize,\n    pub min_value: &'a T,\n}\n\nimpl\u003C'a, T: Ord + Eq> PartialEq for RMQBlockInfo\u003C'a, T> {\n    fn eq(&self, other: &Self) -> bool {\n        self.min_value == other.min_value && self.min_original_idx == other.min_original_idx\n    }\n}\nimpl\u003C'a, T: Ord + Eq> Eq for RMQBlockInfo\u003C'a, T> {}\n\nimpl\u003C'a, T: Ord> PartialOrd for RMQBlockInfo\u003C'a, T> {\n    fn partial_cmp(&self, other: &Self) -> Option\u003Cstd::cmp::Ordering> {\n        Some(self.cmp(other))\n    }\n}\nimpl\u003C'a, T: Ord + Eq> Ord for RMQBlockInfo\u003C'a, T> {\n    fn cmp(&self, other: &Self) -> std::cmp::Ordering {\n        self.min_value.cmp(other.min_value)\n            .then_with(|| self.min_original_idx.cmp(&other.min_original_idx))\n    }\n}\n\nimpl\u003C'a, T: Ord + Hash + Eq> Hash for RMQBlockInfo\u003C'a, T> {\n    fn hash\u003CH: std::hash::Hasher>(&self, state: &mut H) {\n        self.min_value.hash(state);\n        self.min_original_idx.hash(state);\n    }\n}\n\n/// The primary solvers available.\n#[derive(Debug, Clone, Copy)]\npub enum RMQSolverKind {\n    ScanningSolver,\n    DenseTableSolver,\n    SparseTableSolver,\n}\n\ntype BlockLevelSolversMap\u003C'a, T> = HashMap\u003Cusize, Box\u003Cdyn RMQSolver\u003C'a, T> + 'a>>;\n\n/// Represents a solver using the method of four russians.\npub struct FourRussiansRMQ\u003C'a, T: Ord + Hash + Eq + Clone> {\n    static_array: &'a [T],\n    block_size: usize,\n    macro_level_solver: Box\u003Cdyn RMQSolver\u003C'a, RMQBlockInfo\u003C'a, T>> + 'a>,\n    block_level_solvers: BlockLevelSolversMap\u003C'a, T>,\n    macro_array: Vec\u003CRMQBlockInfo\u003C'a, T>>,\n}\n\n\nimpl\u003C'a, T: Ord + Hash + Eq + Clone> FourRussiansRMQ\u003C'a, T> {\n    pub fn new(\n        static_array: &'a [T],\n        block_size: usize,\n        macro_solver_kind: RMQSolverKind,\n        micro_solver_kind: RMQSolverKind,\n    ) -> Self {\n        if block_size == 0 { panic!(\"Block size cannot be zero\"); }\n        let n = static_array.len();\n        let num_blocks = (n + block_size - 1) / block_size;\n\n        let mut macro_array = Vec::with_capacity(num_blocks);\n        for i in 0..num_blocks {\n            let start = i * block_size;\n            let end = std::cmp::min(start + block_size, n) - 1;\n            if start > end { continue; }\n\n            let block_slice = &static_array[start..=end];\n            let block_min_res = get_min_by_scanning(block_slice, start);\n\n            macro_array.push(RMQBlockInfo {\n                start_idx: start,\n                end_idx: end,\n                min_original_idx: block_min_res.min_idx,\n                min_value: block_min_res.min_value,\n            });\n        }\n\n        let macro_level_solver = Self::create_solver(&macro_array, macro_solver_kind);\n\n        let mut block_level_solvers = HashMap::with_capacity(num_blocks);\n        for i in 0..num_blocks {\n            let start = i * block_size;\n            let end = std::cmp::min(start + block_size, n) - 1;\n            if start > end { continue; }\n            let block_slice = &static_array[start..=end];\n            let micro_solver = Self::create_solver(block_slice, micro_solver_kind);\n            block_level_solvers.insert(start, micro_solver);\n        }\n\n        FourRussiansRMQ {\n            static_array,\n            block_size,\n            macro_level_solver,\n            block_level_solvers,\n            macro_array,\n        }\n    }\n\n    fn create_solver\u003CU: Ord + Hash + Eq + Clone + 'a>(\n        data: &'a [U],\n        kind: RMQSolverKind,\n    ) -> Box\u003Cdyn RMQSolver\u003C'a, U> + 'a> {\n        match kind {\n            RMQSolverKind::ScanningSolver => Box::new(ScanningSolver::new(data)),\n            RMQSolverKind::DenseTableSolver => Box::new(DenseTableSolver::new(data)),\n            RMQSolverKind::SparseTableSolver => Box::new(SparseTableSolver::new(data)),\n        }\n    }\n\n    pub fn solve(&self, query_range: &RMQRange\u003C'a, T>) -> RMQResult\u003C'a, T> {\n        let q_start = query_range.start_idx;\n        let q_end = query_range.end_idx;\n\n        if q_start > q_end { panic!(\"Invalid query range\"); }\n\n        let start_block_idx = q_start / self.block_size;\n        let end_block_idx = q_end / self.block_size;\n\n        let mut overall_min_res: Option\u003CRMQResult\u003C'a, T>> = None;\n\n        let mut update_min = |current_min: &mut Option\u003CRMQResult\u003C'a, T>>, new_res: RMQResult\u003C'a, T>| {\n            if let Some(existing_min) = current_min {\n                if new_res.min_value \u003C existing_min.min_value {\n                    *current_min = Some(new_res);\n                }\n            } else {\n                *current_min = Some(new_res);\n            }\n        };\n\n        if start_block_idx == end_block_idx {\n            let block_start = start_block_idx * self.block_size;\n            let solver = self.block_level_solvers.get(&block_start)\n                                .expect(\"Solver for block not found\");\n            let block_internal_range: RMQRange\u003C'a, T> = (q_start, q_end, self.static_array).into();\n            return solver.solve(&block_internal_range);\n        } else {\n            // 1. Handle partial start block\n            let start_block_start = start_block_idx * self.block_size;\n            let start_block_end = std::cmp::min(start_block_start + self.block_size, self.static_array.len()) - 1;\n            if q_start \u003C= start_block_end {\n                let start_solver = self.block_level_solvers.get(&start_block_start).unwrap();\n                let start_range: RMQRange\u003C'a, T> = (q_start, start_block_end, self.static_array).into();\n                let start_res = start_solver.solve(&start_range);\n                update_min(&mut overall_min_res, start_res);\n            }\n\n            // 2. Handle intermediate blocks using macro solver\n            let first_full_block = start_block_idx + 1;\n            let last_full_block = end_block_idx.saturating_sub(1);\n\n            if first_full_block \u003C= last_full_block {\n                let macro_range_obj = RMQRange {\n                    start_idx: first_full_block,\n                    end_idx: last_full_block,\n                    underlying: &self.macro_array,\n                };\n                let macro_min_block_info = self.macro_level_solver.solve(&macro_range_obj);\n                let macro_min_res: RMQResult\u003C'a, T> = (macro_min_block_info.min_original_idx, macro_min_block_info.min_value).into();\n                update_min(&mut overall_min_res, macro_min_res);\n            }\n\n            // 3. Handle partial end block\n            let end_block_start = end_block_idx * self.block_size;\n            if q_end >= end_block_start {\n                let end_solver = self.block_level_solvers.get(&end_block_start).unwrap();\n                let end_range: RMQRange\u003C'a, T> = (end_block_start, q_end, self.static_array).into();\n                let end_res = end_solver.solve(&end_range);\n                update_min(&mut overall_min_res, end_res);\n            }\n        }\n\n        overall_min_res.expect(\"Query range should not be empty if logic is correct\")\n    }\n}\n```\n\nWith the above abstraction in place, we can implement the two-level solution discussed in the preceding section as an instance of the `FourRussiansRMQ` with both `macro_solver` and `micro_solver` set to `ScanningSolver` and $b$ set to $\\sqrt{n}$. We do so below.\n\n```rust\nfn build_sqrt_n_scanner_solver\u003C'a, T: Ord + Hash + Eq + Clone>(\n    static_array: &'a [T]\n) -> FourRussiansRMQ\u003C'a, T> {\n    let n = static_array.len();\n    let block_size = if n == 0 { 1 } else { (n as f64).sqrt().ceil() as usize };\n    let block_size = std::cmp::max(1, block_size);\n\n    FourRussiansRMQ::new(\n        static_array,\n        block_size,\n        RMQSolverKind::ScanningSolver,\n        RMQSolverKind::ScanningSolver\n    )\n}\n```\n\nSo, Block decomposition allowed us to have linear pre-processing time. However, in the process, we lost our constant query time? Can we do better than $O(\\sqrt{n})$ while still maintaining a linear pre-processing time? Yes. We can use a mix of block decomposition and sparse tables to achieve this. Let's see how.\n\n### Hybrid Structures\n\nWhen discussing block decomposition, after decomposing the input into micro arrays, we went ahead and solved the original problem on each block, treating each as a reduced instance of the original. We also did the same for the macro array. In the preceding section, we solved the problem by doing a linear scan. We can, however, use methods from previous sections — sparse and dense lookup tables — to solve the problem on the micro and macro arrays. When we do that, we end up with hybrid solutions that have faster query times. In this section, we shall explore a few hybrid structures and characterize their runtime.\n\nTo create a hybrid structure, we need to decide which method we want to use to solve the problem on the macro array and on each micro array. By mixing and matching methods, we get different hybrids with different runtimes as shown in the table below.\n\n| Block Size | Macro Array Method | Micro Array Method | Runtime (Preproc, Query) |\n|---|---|---|---|\n| $\\lg n$ | Sparse Table | Linear Scan | $\\langle O(n), O(\\lg n)\\rangle$ |\n| $\\lg n$ | Sparse Table | Sparse Table | $\\langle O(n \\lg \\lg n), O(1)\\rangle$ |\n| $\\lg n$ | Hybrid (Sparse Macro / Linear Micro) | Sparse Table | $\\langle O(n), O(\\lg \\lg n)\\rangle$ |\n\nBelow, we implement the first hybrid method ($O(n), O(\\log n)$):\n\n```rust\nfn build_hybrid_lg_n_solver\u003C'a, T: Ord + Hash + Eq + Clone>(\n    static_array: &'a [T]\n) -> FourRussiansRMQ\u003C'a, T> {\n    let n = static_array.len();\n    let block_size = if n == 0 { 1 } else { (usize::BITS - n.leading_zeros()) as usize };\n    let block_size = std::cmp::max(1, block_size);\n\n    FourRussiansRMQ::new(\n        static_array,\n        block_size,\n        RMQSolverKind::SparseTableSolver,\n        RMQSolverKind::ScanningSolver\n    )\n}\n```\n\nBy this point we have a cool and quite efficient algorithm for the offline range min query problem. However, the title of the note did promise an $\\langle O(n), O(1)\\rangle$ solution. We discuss that in the next section with the caveat that the added constant factors that give us asymptotic constant query time may slow down the algorithm in practice. As noted [here](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/01/Small01.pdf), the preceding $\\langle O(n), O(\\log n)\\rangle$ hybrid solution often outperforms the $\\langle O(n), O(1)\\rangle$ solution in practice.\n\n---\n\n### Cartesian Trees & The LCA-RMQ Equivalence\n\nTo fully understand the upcoming $\\langle O(n), O(1)\\rangle$ solution, we need to to first get an intimate understanding of Cartesian Trees. They are largely responsible for the constant time lookup. In this section, we begin by discussing what cartesian trees are and how to efficiently construct them. We then implement a cartesian tree.\n\n#### Cartesian Trees\n\nA cartesian tree is a derivative data structure. It is derived from an underlying array. More formally, the cartesian tree $T$ of an array $A$ is a min binary heap (based on values) of the elements of $A$ organized such that an in-order traversal of the tree yields the original array (based on indices). How can we construct such a tree given some input array? The main observations that will guide our construction will be the requirement that an in-order traversal must yield the array elements in their positional order, and the requirement that the tree be a min heap (parent value \u003C child values).\n\nWe can build the tree [incrementally](https://www.notion.so/A-note-on-algorithmic-design-patterns-20e50d39c99945e3ad8dfb804177ab3f), adding each new element as the rightmost node of the tree being built so far. More specifically, we'll add elements in the order they appear in the array. To add an element $\\chi$ (at index $i$), we inspect the right spine of the partially built tree, starting with the current rightmost node (which corresponds to index $i-1$). We follow parent pointers up the right spine until we find an element, $\\psi$, in the tree that is smaller than $\\chi$. We modify the tree: $\\chi$ becomes the right child of $\\psi$. The node that was previously the right child of $\\psi$ (if any, let's call it $\\omega$) becomes the left child of the new node $\\chi$. If no such smaller element $\\psi$ is found (meaning $\\chi$ is the smallest so far on the right spine), then $\\chi$ becomes the new root of the tree constructed so far, and the old root becomes its left child. Traversing the right spine efficiently can be done using a stack that holds the nodes currently on the right spine. Below, we use this observation to implement a procedure for creating a cartesian tree from some array.\n\n```rust\nuse std::ops::{Index, IndexMut};\nuse std::cmp::Ord;\n\n#[derive(Debug, Ord, PartialOrd, Eq, PartialEq, Clone, Copy)]\nstruct CartesianNodeIdx(usize);\n\n#[derive(Debug)]\nstruct CartesianTreeNode\u003C'a, T: Ord> {\n    value: &'a T,\n    original_idx: usize,\n    left_child_idx: Option\u003CCartesianNodeIdx>,\n    right_child_idx: Option\u003CCartesianNodeIdx>,\n}\n\nimpl\u003C'a, T: Ord> Index\u003CCartesianNodeIdx> for Vec\u003CCartesianTreeNode\u003C'a, T>> {\n    type Output = CartesianTreeNode\u003C'a, T>;\n    fn index(&self, index: CartesianNodeIdx) -> &Self::Output {\n        &self[index.0]\n    }\n}\n\nimpl\u003C'a, T: Ord> IndexMut\u003CCartesianNodeIdx> for Vec\u003CCartesianTreeNode\u003C'a, T>> {\n    fn index_mut(&mut self, index: CartesianNodeIdx) -> &mut Self::Output {\n        &mut self[index.0]\n    }\n}\n\n#[derive(Debug)]\nstruct CartesianTree\u003C'a, T: Ord> {\n    nodes: Vec\u003CCartesianTreeNode\u003C'a, T>>,\n    root_idx: Option\u003CCartesianNodeIdx>,\n    action_profile: Vec\u003CCartesianTreeAction>,\n}\n\n#[derive(Debug, Eq, PartialEq, Clone, Copy)]\nenum CartesianTreeAction {\n    Push,\n    Pop,\n}\n\nimpl\u003C'a, T: Ord> CartesianTreeNode\u003C'a, T> {\n    fn new(value: &'a T, original_idx: usize) -> Self {\n        CartesianTreeNode {\n            value,\n            original_idx,\n            left_child_idx: None,\n            right_child_idx: None,\n        }\n    }\n}\n\n\nimpl\u003C'a, T: Ord> From\u003C&'a [T]> for CartesianTree\u003C'a, T> {\n    fn from(underlying: &'a [T]) -> Self {\n        let len = underlying.len();\n        if len == 0 {\n            return CartesianTree { nodes: Vec::new(), root_idx: None, action_profile: Vec::new() };\n        }\n\n        let mut nodes = Vec::with_capacity(len);\n        let mut stack = Vec::\u003CCartesianNodeIdx>::with_capacity(len);\n        let mut action_profile = Vec::with_capacity(len * 2);\n        let mut root_idx = None;\n\n        for (idx, value) in underlying.iter().enumerate() {\n            let new_node_idx = CartesianNodeIdx(idx);\n            nodes.push(CartesianTreeNode::new(value, idx));\n\n            let mut last_popped_idx = None;\n            while let Some(&top_idx) = stack.last() {\n                if nodes[top_idx].value \u003C nodes[new_node_idx].value {\n                    break;\n                } else {\n                    last_popped_idx = stack.pop();\n                    action_profile.push(CartesianTreeAction::Pop);\n                }\n            }\n\n            if let Some(popped_idx) = last_popped_idx {\n                nodes[new_node_idx].left_child_idx = Some(popped_idx);\n            }\n\n            if let Some(&parent_idx) = stack.last() {\n                nodes[parent_idx].right_child_idx = Some(new_node_idx);\n            } else {\n                root_idx = Some(new_node_idx);\n            }\n\n            stack.push(new_node_idx);\n            action_profile.push(CartesianTreeAction::Push);\n        }\n\n        CartesianTree {\n            nodes,\n            root_idx,\n            action_profile,\n        }\n    }\n}\n\n\nimpl\u003C'a, T: Ord> CartesianTree\u003C'a, T> {\n    fn in_order_traversal(&self) -> Vec\u003C&T> {\n        let mut res = Vec::with_capacity(self.nodes.len());\n        self.traversal_helper(self.root_idx, &mut res);\n        res\n    }\n\n    fn traversal_helper(&self, current_idx_opt: Option\u003CCartesianNodeIdx>, res: &mut Vec\u003C&'a T>) {\n        if let Some(current_idx) = current_idx_opt {\n            let node = &self.nodes[current_idx];\n            self.traversal_helper(node.left_child_idx, res);\n            res.push(node.value);\n            self.traversal_helper(node.right_child_idx, res);\n        }\n    }\n}\n```\n\nYou can play around with the code for constructing a cartesian tree [in the rust playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=c51356cba92f48f0434c64abd21d7162).\n\n---\n\nWhy are cartesian trees important, and how are they related to the `RMQ` problem? First, notice that once we have a cartesian tree for an array, we can answer any `RMQ` on that array. In particular, the minimum value in the range $[i, j]$ of the original array $A$ corresponds to the value of the Lowest Common Ancestor (LCA) of the nodes corresponding to $A[i]$ and $A[j]$ in the Cartesian Tree $T$. That is:\n\n$$\\mathrm{Value}(\\mathrm{RMQ}_A(i, j)) = \\mathrm{Value}(\\mathrm{LCA}_T(\\mathrm{node}(i), \\mathrm{node}(j)))$$ This establishes an equivalence between RMQ and LCA problems. Although this idea is intrinsically interesting, we do not explore it further here via LCA algorithms. Feel free to check out [this note for further details](http://courses.csail.mit.edu/6.851/fall17/scribe/lec15.pdf).\n\nTo fully appreciate the importance of cartesian trees and their relation to the data structure design problem at hand, we have to explore when and how two arrays have isomorphic trees. This will lead us to a way of figuring out when two blocks can share the same pre-processed index — a thing that will lead us to an `RMQ` data structure with constant query time.\n\n#### Cartesian Tree Isomorphisms\n\nWhen do two cartesian trees for two different arrays, $B_1$ and $B_2$ (of the same length), have the same shape (i.e., are isomorphic)? How can we tell this efficiently?\n\nPut simply, two blocks have isomorphic Cartesian trees if and only if the relative order of elements is the same in a way that preserves the min-heap property based on values and the in-order property based on indices. Crucially, if two blocks have isomorphic Cartesian trees, then the *index* of the minimum value within any corresponding sub-range (relative to the start of the block) will be the same for both blocks. This means that the sequence of `Push` and `Pop` operations when constructing the cartesian trees using the stack-based algorithm described earlier will be exactly the same for both blocks.\n\nTherefore, to know if two blocks have isomorphic trees, we could simply compare their `action_profile` vectors. Note, however, that when we are only interested in *whether* two blocks have isomorphic trees, we don't even need to fully construct the tree structure (pointers/indices). We also do not need to allocate space for the action profile vector explicitly if we compute a summary value on the fly. The idea is to create a bitstring from the sequence of `Push` (e.g., bit 1) and `Pop` (e.g., bit 0) operations. The number formed by interpreting this bitstring is called the Cartesian Tree Number. Therefore, with this scheme, two blocks have isomorphic trees if and only if they have the same Cartesian Tree Number. Below, we show how to calculate such a number from the action profile.\n\n```rust\nimpl\u003C'a, T: Ord> CartesianTree\u003C'a, T> {\n    /// Calculates the Cartesian Tree Number of this tree\n    /// using the sequence of `push` (1) and `pop` (0) operations\n    /// stored in the `action_profile`.\n    fn cartesian_tree_number(&self) -> u64 {\n        let mut number: u64 = 0;\n        let mut offset: u32 = 0;\n        if self.action_profile.len() > 64 {\n            panic!(\"Cannot represent Cartesian tree number in u64 for length > 32\");\n        }\n\n        for &action in &self.action_profile {\n            if action == CartesianTreeAction::Push {\n                number |= 1 \u003C\u003C offset;\n            }\n            offset += 1;\n        }\n        number\n    }\n}\n```\n\nA nice consequence of the preceding discussion is that we can determine an upper bound on the number of possible distinct Cartesian tree shapes (and thus distinct Cartesian tree numbers) for an array of a given length $b$. Since the maximum length of the action profile is $2b$, the largest possible Cartesian tree number requires $2b$ bits. Therefore, the number of distinct trees is bounded by $2^{2b} = 4^b$. This number will come in handy when we analyze the runtime of the $\\langle O(n), O(1)\\rangle$ solution.\n\n#### The Fischer-Heun RMQ Structure\n\nHow does all this talk of cartesian trees and cartesian numbers translate into an $\\langle O(n), O(1)\\rangle$ range min query solution? Let's discuss that next.\n\nAs with the other Four Russians methods, we begin by dividing the underlying array into blocks of size $b$. For each of our $\\lceil n/b \\rceil$ blocks, we find the minimum element (value and original index) by scanning ($O(b)$ per block, $O(n)$ total). We then aggregate these block minimums (as `RMQBlockInfo` containing value, original index, block start/end) into the macro array.\n\nTo answer an `rmq` query for $[i, j]$, we identify the start block $B_i$ and end block $B_j$. The query spans potentially three parts: a suffix of $B_i$, a prefix of $B_j$, and several full blocks in between ($B_{i+1}$ to $B_{j-1}$). The minimum could be in any of these parts. We find the minimum in the partial start block (from index $i$ onwards) and the partial end block (up to index $j$) using a block-level solver. We find the minimum among the full intermediate blocks by querying the macro array structure (using indices $i+1$ to $j-1$ relative to the macro array). The final answer is the minimum of these three results.\n\nWe are yet to answer two key questions though: (a) What should our block size $b$ be? and (b) What methods (`SolverKinds`) should we use to answer queries on the macro and micro arrays to achieve $\\langle O(n), O(1)\\rangle$?\n\nWe shall use the `SparseTableSolver` for the macro array. Since the macro array has size $N = \\lceil n/b \\rceil$, preprocessing it takes $O(N \\log N) = O((n/b) \\log(n/b))$ time, and querying takes $O(1)$ time.\n\nFor each micro array (block), we *could* use the `DenseTableSolver`. Preprocessing a single block takes $O(b^2)$ and querying takes $O(1)$. However, doing this for all $n/b$ blocks independently would lead to a total preprocessing time of $O(n) + O((n/b) \\log(n/b)) + O((n/b) \\cdot b^2) = O(n + (n/b)\\log(n/b) + nb)$, which is not linear.\n\nThis is where Cartesian Tree Numbers come in. We calculate the Cartesian Tree Number for each block (size $b$). Blocks with the same number have isomorphic trees, meaning their relative RMQ answers are identical. We only need to build and store one `DenseTableSolver` (precomputing all $O(b^2)$ relative RMQ answers) for each *distinct* Cartesian Tree Number found among the blocks. Since there are at most $4^b$ distinct tree shapes for blocks of size $b$, the total time for preprocessing all unique block types is $O(4^b \\cdot b^2)$. We also need $O(n)$ time to compute the Cartesian number for each of the $n/b$ blocks (assuming $O(b)$ time per block). We store a mapping from each block's starting index (or the block's Cartesian number) to its corresponding precomputed dense table solver.\n\nThe total preprocessing time is now the sum of:\n\n1. Finding block minimums and building macro array: $O(n)$.\n2. Preprocessing macro array with Sparse Table: $O((n/b) \\log(n/b))$.\n3. Calculating Cartesian Tree Numbers for all blocks: $O(n/b \\cdot b) = O(n)$.\n4. Preprocessing unique block types with Dense Table: $O(4^b \\cdot b^2)$.\n\nTotal Preprocessing Time: $T_{pre} = O(n + (n/b)\\log(n/b) + 4^b b^2)$.\n\nThe query time involves: (1) $O(1)$ query on the macro array (Sparse Table). (2) Two $O(1)$ queries on the micro arrays (Dense Tables for start/end blocks, accessed via Cartesian number or block index mapping). (3) Combining these results: $O(1)$.\n\nTotal Query Time: $T_{query} = O(1)$.\n\nOur final task is to pick a value of $b$ that makes the preprocessing time $T_{pre} = O(n)$. We choose $b$ such that $4^b = O(n / \\log n)$ or similar, making the $4^b b^2$ term roughly linear or sub-linear. A common choice is $b = \\frac{1}{4} \\log_2 n = \\frac{1}{2} \\log_4 n$. Let's analyze with $b = c \\log n$ for some constant $c$.\n\n- $(n/b)\\log(n/b) = O((n/\\log n) \\log(n/\\log n)) = O(n)$.\n- $4^b b^2 = 4^{c \\log_2 n} (c \\log n)^2 = (2^{2c})^{\\log_2 n} (c \\log n)^2 = n^{2c} (c \\log n)^2$.\n\nTo make $n^{2c} (\\log n)^2 = O(n)$, we need $2c \\le 1$, so $c \\le 1/2$. Let's choose $c = 1/4$. Then $b = \\frac{1}{4} \\log_2 n$.\n\n- $(n/b)\\log(n/b) \\approx O(n)$.\n- $4^b b^2 = 4^{\\frac{1}{4} \\log_2 n} (\\frac{1}{4} \\log n)^2 = (2^{1/2})^{\\log_2 n} O((\\log n)^2) = n^{1/2} O((\\log n)^2) = O(\\sqrt{n} (\\log n)^2)$, which is $o(n)$.\n\nTherefore, choosing $b = \\frac{1}{4} \\log_2 n$ makes the total preprocessing time $O(n)$.\n\nTo summarize, by choosing a block size of $b = \\frac{1}{4} \\log n$, using a `SparseTableSolver` for the macro array, and shared `DenseTableSolver` instances for micro blocks (cached based on Cartesian Tree Numbers), we achieve $\\langle O(n), O(1) \\rangle$ complexity.\n\nThus, our final data structure has the following features:\n\n| Block Size | Macro Array Method | Micro Array Method | Runtime (Preproc, Query) |\n|---|---|---|---|\n| $\\frac{1}{4} \\log n$ | Sparse Table | Dense Table (shared via Cartesian Tree Numbers) | $\\langle O(n), O(1)\\rangle$ |\n\nAs discussed earlier, although this method has impressive asymptotic numbers, it is often outperformed in practice by the hybrid with logarithmic query time due to larger constant factors and implementation complexity. Furthermore, this method is a lot more complex. That is another reason, from an engineering standpoint, to prefer the $\\langle O(n), O(\\log n)\\rangle$ method — much less code, and often just as fast in practice.\n\nWe leave the implementation of this final $\\langle O(n), O(1)\\rangle$ scheme as an exercise. Using the abstractions from above, the implementation should be a logical extension. One needs to add logic to compute Cartesian Tree Numbers for each block, manage a map from the number to a precomputed dense solver, and use this map during the query phase for the start and end blocks.\n\n---\n\n## References\n\n- [CS 166 Lecture 1](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/00/Small00.pdf)\n- [CS 166 Lecture 2](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/01/Small01.pdf)\n- [MIT 6.851 Lecture Notes (related concepts)](http://courses.csail.mit.edu/6.851/fall17/lectures/L15.pdf)","src/content/writing/rmq.mdx","39f9f014ccc5b07d","rmq.mdx","sketch",{"id":67,"data":69,"body":76,"filePath":77,"digest":78,"legacyId":79,"deferredRender":17},{"title":70,"date":71,"description":72,"tags":73,"draft":60},"Streaming Algorithms: Sampling and Sketching",["Date","2023-05-01T00:00:00.000Z"],"Implementing reservoir sampling, bloom filters, count-min sketch, HLL, and the Johnson-Lindenstrauss transform in Rust.",[57,74,59,75],"sketching","probabilistic","All algorithms operate on some data. Traditional algorithms assume that they have full access to the data they need in order to solve the problem at hand. For instance, algorithms for sorting a collection of items naturally assume that they have access to a buffer holding all the items. Similarly, procedures for computing the convex hull of a set of points assume that they have all the points. In some applications, however, the full set of data is not known a priori. In this *online setting*, algorithms have to provide useful answers to queries before having the chance to see all the data.\n\nThe way such algorithms provide their answers depends on the kind of query being asked. Queries can be `ad hoc` meaning that we do not know the exact query we'd like to run a priori. In this case, the most common technique used is `random sampling`. We can also have `standing queries`. In this case, we know exactly the queries that we'd like to answer. Because we know the queries beforehand, we can design specialized data structures capable of providing useful answers to our queries.\n\nThese two cases share two key characteristics:\n\n1. The answers that they produce will be approximate — not exact.\n2. They do not seek to store and process all the incoming data. Instead, their goal is to quickly process each observation to create a summary that they can use to answer either standing or ad hoc queries. This summary is often referred to as a Sketch of the data.\n\nIn this note, we discuss and implement several ideas that are useful when designing online algorithms. We begin by discussing a classic method for selecting a representative sample from a stream of data. We then discuss the idea that is foundational to most sketching algorithms and show how it's employed to improve the accuracy of approximate online methods. Finally, we discuss a few key algorithms for answering some standing queries.\n\n## Reservoir Sampling\n\nReservoir Sampling is a simple procedure for picking a random uniform sample of size `k` from a datastream whose size is unknown (it could be infinite). This scheme works as follows: We store the first `k` items into a buffer of size k. Then for each element that arrives afterwards, with probability $\\dfrac{k}{i}$ we decide to keep the $i_{th}$ sample. If we keep it, we pick an element, uniformly at random, i.e with probability $\\dfrac{1}{k}$ to evict from the buffer and replace it with the new sample. We implement this scheme below.\n\nFirst, since we may be implementing other samplers later on, we introduce a trait that encapsulates the behavior that we expect from any sampler. Put simply, a sampler is anything that can observe a stream of events and produce a sample from that stream at any time.\n\n```rust\nuse rand::{thread_rng, Rng};\n\n/// A sampler will be anything that can observe a possibly infinite\n/// number of items and produce a finite random sample from that\n/// stream\npub trait Sampler\u003CT> {\n    /// We observe each item as it comes in\n    fn observe(&mut self, item: T);\n\n    /// Produce a random uniform sample of the all the items that\n    /// have been observed so far.\n    fn sample(&self) -> &[T];\n}\n```\n\nWith that out of the way, we can now introduce the reservoir sampling data structures and procedures. The reservoir sampler only needs to keep track of two values: the number of events that have been observed so far and the requested sample size.\n\n```rust\n#[derive(Debug)]\npub struct ReservoirSampler\u003CT> {\n    /// This is what we produce whenever someone calls sample. It\n    /// maintains this invariant: at any time-step `t`, reservoir\n    /// contains a uniform random sample of the elements seen thus far\n    reservoir: Vec\u003CT>,\n\n    /// This determines the size of the reservoir. The\n    /// client sets this value\n    sample_size: usize,\n\n    /// The number of items we have seen so far. We use this\n    /// to efficiently update the reservoir in constant time\n    /// while maintaining its invariant.\n    count: usize,\n}\nimpl\u003CT> ReservoirSampler\u003CT> {\n    /// Create a new reservoir sampler that will produce a random sample\n    /// of the given size.\n    pub fn new(sample_size: usize) -> Self {\n        ReservoirSampler {\n            reservoir: Vec::with_capacity(sample_size),\n            sample_size,\n            count: 0,\n        }\n    }\n}\n```\n\nFinally, we turn our `ReservoirSampler` object into a sampler by implementing the appropriate trait.\n\n```rust\nimpl\u003CT> Sampler\u003CT> for ReservoirSampler\u003CT> {\n    fn observe(&mut self, item: T) {\n        // To make sure we that maintain the reservoir invariant,\n        // we have to ensure that each incoming item has an equal\n        // probability of being included in the sample. We do so by\n        // generating a random index `k`, and if `k` falls within\n        // our reservoir, we replace the item at `k` with the\n        // new item\n        if self.reservoir.len() == self.sample_size {\n            let rand_idx = thread_rng().gen_range(0..self.count);\n            if rand_idx \u003C self.sample_size {\n                self.reservoir[rand_idx] = item;\n            }\n        } else {\n            // If the reservoir is not full, no need\n            // to evict items\n            self.reservoir.push(item)\n        }\n        self.count += 1;\n    }\n\n    fn sample(&self) -> &[T] {\n        // Since we always have a random sample ready to go, we\n        // simply return it\n        self.reservoir.as_ref()\n    }\n}\n```\n\nYou can find runnable code for the reservoir sampling procedure [in the playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=70732619db2d901ea9bdf832793a9563). One final note: The sampling scheme we implemented, while easy to understand, can be a bottleneck in applications where speed is a priority. The slowdown comes from the fact that hashing is an expensive operation, and our procedure hashes each item it encounters — even those that it eventually discards. Take a look at [the wikipedia page](https://en.wikipedia.org/wiki/Reservoir_sampling) for sampling schemes that improve upon the procedure above.\n\n## Foundational Ideas\n\nBefore we begin discussing strategies for answering standing queries on streaming data, let's first lay the foundation by exploring two key ideas. These ideas are the major motifs that you'll encounter in almost all sketching algorithms.\n\n### Hashing\n\nAlthough hashing is mostly known as the first key ingredient in the design of hash tables, it is also quite useful when designing probabilistic sketching algorithms. In this setting, we use hashing to introduce randomness. To see why randomness is important, consider an alternate method for sampling `k` items from a possibly infinite data stream. We could accomplish this by tagging each incoming item with a random number then storing the items and their tags in a bounded heap. Once the heap reaches capacity, we evict the item with the smallest tag before adding in a new `\u003Citem, tag>` pair.\n\nAt this point, we have a vague idea of why hashing is a useful idea. But, what is hashing — really? [Put simply](https://gankra.github.io/blah/robinhood-part-1/#hashing), hashing takes an arbitrary input (often as a collection of bytes) and transforms it, using a hash function, into a random integer. How is this different from tagging each input with a random number as we did above? Well, if the hash function is *well constructed*, and the functions co-domain is of size `m`, then the probability that two distinct items receive the same tag is $\\frac{1}{m}$. A hash function that provides this guarantee is called a universal hash function. In this note, we assume that all our hash functions are universal.\n\nIn addition to universality, there are other details that are cool to know about hash functions. First, there exists a tradeoff between the speed of a hash function and its security guarantees. Second, when hashing is used as a component of systems that have exposure to potential adversaries, it as an attack vector — via hash-flooding attacks. This is why we almost always use a randomly initialized hash function. We won't make use of these last two ideas in this note, so we don't discuss them further.\n\nBelow, we explore the hashing interface provided by the rust programming language.\n\n```rust\n//! Rust exposes hashing utilities via a Streaming Hasher interface. That is, you\n//! feed arbitrary bytes into the hash function, then, when done, you ask\n//! the hasher to produce a hash value.\n//!\n//! use std::hash::{Hash, Hasher};\n\n/// The Hasher trait from the standard library. This is implemented\n/// by hash functions\npub trait Hasher {\n    /// Writes the given list of bytes into this hasher. This usually\n    /// updates some internal state. Because the hasher is stateful,\n    /// we can keep on calling `write` until we've exhausted all our\n    /// bytes\n    fn write(&mut self, bytes: &[u8]);\n\n    /// Produces the hash value of the bytes fed into this hasher so far\n    /// without resetting the hasher's internal state. This means that\n    /// if we want a hash value for a different stream of bytes, we have to\n    /// create a new hasher instance.\n    fn finish(&self) -> u64;\n\n    // ... snip\n}\n\n/// The Hash trait from the standard library. This is implemented by hashable\n/// types\npub trait Hash {\n    /// Feed the value into a given Hasher\n    fn hash\u003CH: Hasher>(&self, state: &mut H);\n\n    // ... snip\n}\n```\n\nThe long and short of it is, we create a `hasher` from some random state — `random_state.build_hasher()`. Then, given an item that implements the `Hash` trait, we call its `hash` function passing in the hasher — `item.hash(hasher)`. Then to extract the hash value, we call the `finish` method of the hasher. The tricky bit to remember is that calling finish does not reset the hasher's internal state. As we'll see later on, this means that we have to create a new `hasher` instance to hash a fresh item.\n\n### Fingerprints & Probability Amplification\n\nOnce we apply a hash function to an item, we can use the generated hash value to *almost* uniquely identify that object. The hash value can thus be thought of as a fingerprint of the initial object in that it is a relatively unique and lightweight identifier of the object — just like human fingerprints. Because of hash collisions, however, it is not fully unique. When designing probabilistic data structures, we strive to reduce the likelihood of two distinct items colliding. That is where probability amplification comes in. Instead of simply hashing an item once, we rerun the hashing experiment `k` times, **each time with a different, independent, random hash function**.\n\nNow, the fingerprint is composed of the `k` hash values. This scheme dramatically reduces the probability of fingerprint collision. In particular, the probability that two fingerprints from two different objects are the same is $\\frac{1}{m^k}$, where `m` is the size of the co-domain of our hash functions (this is often the number of buckets we're hashing our items into).\n\nTo really appreciate how powerful these two ideas are, let's look at a quick example borrowed from [CS 168](https://web.stanford.edu/class/cs168/l/l4.pdf). Suppose we have `n` objects from a universe `U`. Each object requires $\\lg_2 |U|$ bits to uniquely identify. For instance, if our universe is the address space on a 64 bit machine, each address would require $\\lg_2 2^{64} = 64$ bits. Now, suppose that we feel that `64` bits are too many, and thus want to represent each address using fewer bits. How could we go about this?\n\nTo recapitulate, we can represent arbitrary objects using their hash values. These values are often smaller (e.g 8 bytes) than the underlying objects. Furthermore, instead of simply using a single hash value, we can use a collection of `k` hash values each produced by `k` independent hash functions.\n\n## Bloom Filters\n\nA Bloom Filter 🥦 is a compact data structure that summarizes a set of items, allowing us to answer membership questions — has a certain item been seen before? Unlike other set structures — such as hash sets and search trees — a bloom-filter's answer to a membership query is either a `definite NO` or a `Probable Yes`. That is, it can have false positives — telling us that an item has been seen when in fact it has never been observed. Therefore, Bloom filters are best suited for cases where false positives can be tolerated and mitigated. Cases where the effect of a false positive is not a wrong program state but extra work.\n\nBelow, we introduce the interface shared across all filters capable of answering approximate membership queries.\n\n```rust\n/// Indicates that the filter has probably seen a given\n/// item before\npub struct ProbablyYes;\n\n/// Indicates that a filter hasn't seen a given item before.\npub struct DefinitelyNot;\n\n/// A filter will be any object that is able to observe a possibly\n/// infinite stream of items and, at any point, answer if a given\n/// item has been seen before\npub trait Filter\u003CT> {\n    /// We observe each item as it comes in. We do not use terminology such as\n    /// `insert` because we do not store any of the items.\n    fn observe(&mut self, item: T);\n\n    /// Tells us whether we've seen the given item before. This method\n    /// can produce false positives. That is why instead of returning a\n    /// boolean, it returns `Result\u003CProbablyYes, DefinitelyNot>`\n    fn has_been_observed_before(&self, item: &T) -> Result\u003CProbablyYes, DefinitelyNot>;\n}\n```\n\nA Bloom Filter is parameterized by two values: a bit array `buckets` with `m` buckets in it, and a set of `k` hash functions $\\{h_1, h_2, \\ldots h_k\\}$. Given an input `item`, we apply each of our `k` hash functions to it and then set all the indexes that `item` hashed to to `true`. To check if an item has already been seen, we again first apply each of our `k` hash functions and check if **all** corresponding locations to are `true`.\n\nNotice how a bloom filter is simply a direct application of object fingerprinting and probability amplification. We hash each item using multiple hash functions to reduce the chances of getting a false positive.\n\nBelow, we provide an implementation of a bloom filter.\n\n```rust\nuse std::hash::{Hash, Hasher};\nuse std::collections::hash_map::RandomState;\nuse std::marker::PhantomData;\n\n#[derive(Debug)]\npub struct BloomFilter\u003CT: Hash> {\n    /// The bit vector. The number of buckets is determined\n    /// by the client\n    buckets: Vec\u003Cbool>,\n\n    /// The list of hash functions. Again, the number of\n    /// hash functions is determined by the client\n    hash_functions: Vec\u003CRandomState>,\n\n    /// This is here to make the compiler happy. We'd\n    /// like for our filter to be parameterized by\n    /// the items it's monitoring. However, we do not\n    /// store those items in the filter.\n    _marker: PhantomData\u003CT>,\n}\n\nimpl\u003CT: Hash> BloomFilter\u003CT> {\n    /// Creates a new bloom filter with `m` buckets and `k` hash functions.\n    /// Each hash function is randomly initialized and is independent\n    /// of the other hash functions\n    pub fn new(m: usize, k: usize) -> Self {\n        // initialize all the bucket locations to false\n        let mut buckets = Vec::with_capacity(m);\n        for _ in 0..m {\n            buckets.push(false);\n        }\n\n        // initialize the hash functions randomly.\n        let mut hash_functions = Vec::with_capacity(k);\n        for _ in 0..k {\n            hash_functions.push(RandomState::new());\n        }\n\n        BloomFilter {\n            buckets,\n            hash_functions,\n            _marker: PhantomData,\n        }\n    }\n\n    /// This performs the actual hashing.\n    fn get_index(&self, state: &RandomState, item: &T) -> usize {\n        let mut hasher = state.build_hasher();\n        item.hash(&mut hasher);\n        let idx = hasher.finish() % self.buckets.len() as u64;\n        idx as usize\n    }\n}\n```\n\nWith the above abstractions in place, we can go ahead and implement the core filter procedures.\n\n```rust\nimpl\u003CT: Hash> Filter\u003CT> for BloomFilter\u003CT> {\n    /// As already explained, we apply each of our `k` hash functions to the\n    /// given item and set the locations it hashes to to true\n    fn observe(&mut self, item: T) {\n        for state in &self.hash_functions {\n            let index = self.get_index(state, &item);\n            self.buckets[index] = true;\n        }\n    }\n\n    /// Again, we start by applying each of our `k` hash functions\n    /// to the given item then. If all resultant `k` locations are `true`,\n    /// we conclude that we MIGHT have observed this item. If any location\n    /// is false, we immediately conclude that we NEVER saw this item\n    fn has_been_observed_before(&self, item: &T) -> Result\u003CProbablyYes, DefinitelyNot> {\n        for state in &self.hash_functions {\n            let index = self.get_index(state, &item);\n            if !self.buckets[index] {\n                return Err(DefinitelyNot);\n            }\n        }\n        Ok(ProbablyYes)\n    }\n}\n```\n\n## The Count-Min Sketch\n\nCount-Min Sketch 🍅 is a compact structure for estimating the counts of each type of items in our set. A naive way of solving the count each problem would be to allocate an integer counter for each class of items. However, this may not be practical when the number of item types grows huge. The CMSketch provides a tradeoff between count accuracy and low memory usage — it encodes a potentially massive number of item types in a small array, guaranteeing that large counts will be preserved fairly accurately while small counts may incur greater relative error. Count-Min sketches are thus best suited to cases where a slight inflation in frequency does not lead to an illegal program state.\n\nAs with the bloom filter, we use multiple hash functions. However, unlike with the bloom filter where all the functions hashed to the same array, each function in this case has its own dedicated set of buckets. A CMSketch is thus a matrix with as many rows as the number of hash functions and as many columns as the number or buckets. When we see an item, we apply our `k` hash functions to it and increment the slots that it maps to in each row of the sketch. To estimate the count of an item, we apply our hash functions and read the values at the slots that it maps to and then select the minimum out of these. One cool thing to note is that the accuracy guarantees of the CMSketch are not in any way related to the size of the dataset — they depend entirely on the number of hash functions `k` and the number of buckets `m`.\n\n```rust\nuse std::hash::{Hash, Hasher};\nuse std::collections::hash_map::RandomState;\nuse std::marker::PhantomData;\nuse std::mem::{self, MaybeUninit};\n\n\n#[derive(Debug)]\npub struct CountMinSketch\u003CT: Hash, const M: usize, const N: usize> {\n    /// A count sketch is defined by `N` hash functions\n    /// and `M` bucket groups. Each of the `N` hash functions\n    /// maps an item into a single slot in a corresponding bucket group\n    sketch_matrix: [[u64; M]; N],\n\n    ///\n    hash_functions: [RandomState; N],\n\n    /// As with the Bloom Filter, we'd like for our sketch to be\n    /// parameterized by the type of objects it is counting\n    _marker: PhantomData\u003CT>\n}\n\nimpl \u003CT: Hash, const M: usize, const N: usize> CountMinSketch\u003CT, M, N> {\n    /// Create a new instance of the CMSketch\n    pub fn new() -> Self {\n        let hash_functions = unsafe {\n            let mut hash_function_slots: [MaybeUninit\u003CRandomState>; N] = MaybeUninit::uninit().assume_init();\n            for slot in &mut hash_function_slots {\n                slot.write(RandomState::new());\n            }\n            mem::transmute_copy::\u003C_, [RandomState; N]>(&hash_function_slots)\n        };\n\n        CountMinSketch {\n            sketch_matrix: [[0; M]; N],\n            hash_functions,\n            _marker: PhantomData::default(),\n        }\n    }\n\n    /// Increment the count of the given item\n    pub fn inc(&mut self, item: &T) {\n        for (i, state) in self.hash_functions.iter().enumerate() {\n            let idx = self.get_index(state, item);\n            self.sketch_matrix[i][idx] += 1;\n        }\n    }\n\n    /// Retrieve the approximate count of the given item\n    pub fn count(&self, item: &T) -> u64{\n        let mut cur_min = u64::MAX;\n        for (i, state) in self.hash_functions.iter().enumerate() {\n            let idx = self.get_index(state, item);\n            let cur_value = self.sketch_matrix[i][idx];\n            if cur_value \u003C cur_min {\n                cur_min = cur_value;\n            }\n        }\n        cur_min\n    }\n\n    /// Hashes the given item and maps it to the appropriate index location within\n    /// a single bucket\n    fn get_index(&self, state: &RandomState, item: &T) -> usize {\n        let mut hasher = state.build_hasher();\n        item.hash(&mut hasher);\n        let idx = hasher.finish() % self.sketch_matrix[0].len() as u64;\n        idx as usize\n    }\n}\n```\n\n## Approximate Counting with The Hyper-Log-Log\n\n> When in doubt, count. But, what if we do not have enough scratch paper? What should we do in that case? Well, in that case, we still count, but do so while being probably approximately correct.\n\nThe hyper log log (**HLL**) data structure is used to estimate the number of **distinct**/**unique** items present in some large collection of items with duplicates. That is, it is used to estimate the **Cardinality of a Multiset**.\n\nIf our data is such the number of unique items can fit in memory, then this so called, `Count Distinct` problem can easily be solved by maintaining a counter and a `HashSet` of elements that have already been observed.\n\nIn the streaming setting, we cannot afford to do that as the incoming data could even have infinite cardinality. Consider this:\n\n> You are running a popular web service and you'd like to keep track of the number of unique visitors to your site. Each visitor is uniquely identified by their IP address. Suppose your service has $10^9$ unique visitors. Using the `HashSet` approach, you'd need between 4GB for `IPV4` addresses and `16GB` for `IPV6` addresses.\n\nThe HLL is able to solve this problem in a much more space efficient manner by trading off accuracy. In the following sections, we'll demonstrate how to construct the HLL structure using Rust.\n\nAs usual, we begin by defining our interface.\n\n```rust\n/// A cardinality estimator will be any object that is able to observe a possibly\n/// infinite stream of items and, at any point, estimate the number of unique items\n/// seen so far.\npub trait CardinalityEstimator\u003CT> {\n    /// We observe each item as it comes in.\n    fn observe(&mut self, item: &T);\n\n    /// Return an estimation of the number of unique items see thus far\n    fn current_estimate(&self) -> u64;\n}\n```\n\nWe are going to implement three versions of probabilistic cardinality estimators. The first one, the naïve estimator, although easy to implement, will result in estimates with fairly high variance. The second one will improve upon this by making the observation that \"interesting\" patterns in our hash values (such as a trail of all zeroes) are rare enough that we can use their presence as a proxy for how many unique hash values we've observed. The final estimator will be based on the second. It'll simply introduce key optimization to improve the second's efficiency.\n\n### A Naïve Cardinality Estimator\n\n```rust\nuse std::marker::PhantomData;\n\n/// A naïve estimator that estimates the cardinality\n/// to simply be 1 divided by the smallest hash value\n/// observed thus far, after mapping it to number\n/// between 0 and 1\npub struct NaiveEstimator\u003CT> {\n    // The smallest hash value seen so far.\n    cur_hash_value_min: Option\u003Cu64>,\n    /// We want our estimator to be parameterized by the type\n    /// it is estimating, but we don't need to store anything\n    /// so we never use the provided type. To keep the compiler\n    /// happy, we sacrifice a single ghost as an offering\n    _marker: PhantomData\u003CT>,\n}\n```\n\n### An Improved Cardinality Estimator\n\n```rust\n/// Estimate the number of distinct items in the stream as\n/// `2^k` where `k` is the longest trail of leading zeroes\n/// among the hash values of all the items observed so far\npub struct MaxTrailEstimator\u003CT> {\n    // The smallest hash value seen so far.\n    cur_max_trail_length: Option\u003Cu64>,\n    /// We want our estimator to be parameterized by the type\n    /// it is estimating, but we don't need to store anything\n    /// so we never use the provided type. To keep the compiler\n    /// happy, we sacrifice a single ghost as an offering to the\n    /// compiler gods\n    _marker: PhantomData\u003CT>,\n}\n```\n\n### The HLL Estimator\n\n```rust\n/// WIP\n```\n\n### A word on HLL+\n\n```rust\n/// WIP\n```\n\n## The Johnson-Lindenstrauss Transform\n\n```rust\n/// WIP\n```\n\nEuclidean Distance, also known as the $\\ell_2$ distance, is the most common distance metric. It is defined over vectors of real values. It is defined as $D_\\epsilon (x, y) = ||x - y||_2 = \\left( \\sum_{i = 1}^{d} [x_i - y_i]^2 \\right)^{0.5}$ for points $x \\in \\mathbb{R}^d, y \\in \\mathbb{R}^d$. This can be generalized by replacing 2 with arbitrary values $p$.\n\nThe Curse of Dimensionality is the observation that, the number of neighbors of any point $p$ is related exponentially to the number of dimensions of our space. In $\\mathbb{R}^1, \\mathbb{R}^2, \\mathbb{R}^3$ we have $p = 2^1, 2^2, 2^3$ respectively. In general, $p = 2^d$ for $\\mathbb{R}^d$. This grows so large for moderate values of $d$. Since there is no known way to overcome this curse, we have to resort to using approximate methods.\n\nAt the heart of approximate nearest neighbor methods lies Dimensionality Reduction. The goal is to project our high dimensional data into a low dimensional space while preserving inter-point distance as much as possible. This would allows have our cake and eat it too — we'd be able to richly represent our data in high dimensions and leverage dimensionality reduction to map our data and queries down to a small number of dimensions so as to overcome — approximately, the curse of dimensionality.\n\nTo motivate dimensionality reduction, it is instructive to look at two key foundational techniques — fingerprinting and probability amplification: Suppose we have a set of $n$ objects from a universe $\\mathcal{U}$. Note that we need $\\log_2|\\mathcal{U}|$ bits to distinctly represent any object from this universe. For large $\\mathcal{U}$ this may be too many bits. Can we distinctly represent each object with fewer bits? One simple scheme is to use a single bit using the mapping $f(x) = h(x) \\mod 2$. If the hash function is good, we expect $x = y \\rightarrow f(x) = f(y)$, i.e equality to be preserved and when $x \\neq y$, then $\\Pr[f(x) = f(y)] \\leq 0.5$, i.e distinctness is preserved 50% of the time. We can improve this last error rate by repeating the hashing experiment $k$ times using $k$ independent hash functions and label each object using $k$ bits — this bitstring is the fingerprint of the original object. This is probability amplification. It dramatically reduces the error rate. With 2 hash functions, the error rate drops down to $0.25$. In general, the error rate is $2^{-k}$.\n\nRandom Projections is an extension of the idea of fingerprinting to approximately preserve the $L_2$ distance between object pairs. Suppose we have $n$ objects of interest, $x_1 \\ldots x_n$ in $k$-dimensional space. We can choose a random vector $r = (r_1 \\ldots r_k)$ from the same space. If we take the inner product between $r$ and any of our objects, we'll get a single number $f_r(x_i) = \\langle x_i, r\\rangle = x^Tr$. We have thus mapped a k-vector to a single real value that is a random linear combination of the elements in the original vector. This is the random projection of the vector.\n\nThe value in our random vector can be sampled iid from a standard gaussian. If we do that, the projections of any two vectors will be an unbiased estimate of the euclidean distance between the two vectors. As usual, we can use the magic of independent trials to increase the \"accuracy\" of our projections. To do so, instead of picking a single random vector, we pick $d$ random vectors and average the random projections we get from them. Averaging $d$ independent unbiased estimates yields an unbiased estimate and drops the variance of our estimate by a factor of $d$.\n\nThe Johnson-Lindenstrauss transform (JL) generalizes this idea. It is defined by a $d \\times k$ matrix $A$ composed of our $d$ random vectors. The matrix maps vectors in $\\mathbb{R}^k$ to vectors in $\\mathbb{R}^d$ using the mapping $x_d = \\dfrac{1}{\\sqrt d}Ax$. We can apply the JL-transform whenever we have high dimensional data and are working on a computation that only cares about euclidean distance. In that case, there's little loss in doing the computation in $d$-dimensional space.\n\n## Analysis Tools\n\nIn the preceding sections, we cursorily talked about the \"guarantees\" of our different methods. In order to fully formalize the correctness and efficiency guarantees of probabilistic algorithms, we often turn to two basic tools:\n\n### Markov's Inequality\n\n`Markov's Inequality` tells us that the probability that a random variable, $X$, is $\\lambda$ times the average value is at most $\\dfrac{1}{\\lambda}$. That is\n\n$$P \\left( X \\geq \\lambda E[X] \\right) \\leq \\frac{1}{\\lambda}$$\n\n### Chebyshev's Inequality\n\n`Chebyshev's Inequality` tells us that the probability that a random variable $X$ is more than $\\beta$ standard deviations from its mean is at most $\\dfrac{1}{\\beta ^2}$. That is\n\n$$P \\left( |X - E[X]| \\geq \\beta \\sqrt{Var(X)} \\right) \\leq \\dfrac{1}{\\beta^2}$$\n\n## References\n\n1. [CS 168 Lecture 2](https://web.stanford.edu/class/cs168/l/l2.pdf)\n2. [CS 168 Lecture 4](https://web.stanford.edu/class/cs168/l/l4.pdf)\n3. [This Survey Paper](http://dimacs.rutgers.edu/~graham/pubs/papers/cacm-sketch.pdf)\n4. [This Book by Jelani Nelson](https://www.sketchingbigdata.org/fall20/lec/notes.pdf)\n5. [This Talk by Nicholas Ormrod](https://www.youtube.com/watch?v=YA-nB2wjVcI&ab_channel=CppCon)\n6. [This Paper from Google](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40671.pdf)\n7. [This Blog Post](http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation) and [this other one from Facebook](https://engineering.fb.com/2018/12/13/data-infrastructure/hyperloglog/)","src/content/writing/sketch.mdx","3927bee661d6c8a7","sketch.mdx","spectral",{"id":80,"data":82,"body":18,"filePath":86,"digest":87,"legacyId":88,"deferredRender":17},{"title":83,"date":84,"tags":85,"draft":17},"Spectral Analogs for Nonlinear Maps: Local Linearization & Koopman Operator Theory",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/writing/spectral.mdx","bf1b0f7c706c88b8","spectral.mdx","stochastic-estimation",{"id":89,"data":91,"body":18,"filePath":95,"digest":96,"legacyId":97,"deferredRender":17},{"title":92,"date":93,"tags":94,"draft":17},"Stochastic Estimation: An Overview",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/writing/stochastic-estimation.mdx","7422353418ee27df","stochastic-estimation.mdx","interp",{"id":98,"data":100,"body":28,"filePath":104,"digest":105,"legacyId":106,"deferredRender":17},{"title":101,"date":102,"tags":103,"draft":17},"Thoughts on Interpreting Neural Networks",["Date","2023-08-01T00:00:00.000Z"],[],"src/content/writing/interp.mdx","aedff07a2f2bfe0c","interp.mdx","leitmotifs",{"id":107,"data":109,"body":18,"filePath":113,"digest":114,"legacyId":115,"deferredRender":17},{"title":110,"date":111,"tags":112,"draft":17},"Leitmotifs in Analysis, Algebra, and Stochastic Theory",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/writing/leitmotifs.mdx","e1e7eb04cf64967d","leitmotifs.mdx","str-indexing",{"id":116,"data":118,"body":126,"filePath":127,"digest":128,"legacyId":129,"deferredRender":17},{"title":119,"date":120,"description":121,"tags":122,"draft":60,"citationKey":124,"citationUrl":125},"String Indexing: A Rust Implementation",["Date","2023-07-01T00:00:00.000Z"],"Building suffix arrays and LCP arrays in Rust, from a naïve O(n log n) sort to the linear-time SA-IS algorithm.",[57,123,59,58],"strings","jlikhuva2021string_indexing","https://github.com/jlikhuva/blog/blob/main/posts/string_indexing.md","import TheoremBox from '../../components/TheoremBox.astro';\nimport AlgorithmBox from '../../components/AlgorithmBox.astro';\nimport Sidenote from '../../components/Sidenote.astro';\n\nIn this article, we discuss two structures that are foundational to tasks that need to do substring matching: The Suffix Array and The Longest Common Prefix Array. We'll explore two linear time procedures (SA-IS & Kasai's Algorithm) for constructing these data structures given some underlying, fairly static string from some alphabet $\\Sigma$.\u003CSidenote n={1}>Another name for an alphabet is a Vocabulary.\u003C/Sidenote>\n\n## Substring Search\n\nWe begin by pinning down what it means for a string to be a substring of another.\n\n\u003CTheoremBox type=\"definition\" title=\"Substring\">\n  \u003CFragment slot=\"given\">\n    - A string $\\alpha$\n    - Another string $\\beta$\n  \u003C/Fragment>\n  \u003CFragment slot=\"where\">\n    - Both strings belong to the same alphabet $\\Sigma$\n    - $|\\alpha| = m \\leq |\\beta| = n$\n    - There exists some suffix of $\\beta$, call it $\\beta_{[k:]}$, such that $\\alpha$ is a prefix of $\\beta_{[k:]}$\n  \u003C/Fragment>\n  \u003CFragment slot=\"then\">\n    We say that $\\alpha$ is a substring of $\\beta$\n  \u003C/Fragment>\n\u003C/TheoremBox>\n\nThat the notion of a substring relies, in its definition, on the notion of a suffix explains why we would want to construct a suffix array if our goal is to do substring matching. But, before we proceed any further, we need to start from the beginning by explaining what a suffix array is.\n\nA suffix array for some string $\\Omega$ is an array of all the suffixes of that string. Note that, since a suffix is fully defined by its starting index in the string, there are $m + 1$ possible suffixes (where the extra 1 comes from counting the empty suffix). This array is sorted lexicographically. To save space, suffix arrays do not store the full suffixes, instead, they only store the starting index of each suffix. To sum up, a suffix array for some string is a lexicographically sorted array of all the indexes of all suffixes in the underlying string.\n\n\u003CAlgorithmBox title=\"Example Algorithm: Enhanced Binary Search\">\n\n**Inputs:** A sorted array $A$ of $n$ elements. A target value $T$.\n\n**Outputs:** Index $i$ if $A[i] = T$, else -1.\n\n**Procedure:**\n1. Set $L=0, R=n-1$.\n2. While $L \\leq R$:\n   - $m = \\lfloor(L+R)/2\\rfloor$.\n   - If $A[m] \u003C T$, $L=m+1$.\n   - Else if $A[m] > T$, $R=m-1$.\n   - Else return $m$.\n3. Return -1.\n\n**Notes:** Complexity: $O(\\log n)$. Array must be sorted.\n\n\u003C/AlgorithmBox>\n\nFinally, most algorithms that operate on strings assume that each string has a sentinel character appended at the end. This sentinel character should not appear anywhere else in the string and should be lexicographically smaller than all characters that could appear in the string. For the ASCII alphabet, we often use the character `$` as the sentinel.\n\n## The Suffix Array: A Naïve Solution via Sorting\n\nHow can we create the suffix array? The most straightforward way would be to first generate all possible suffixes, then sort them lexicographically. This runtime of this approach is at least `m lg m` as that is the comparison-based sorting lower bound.\n\nFor instance, suppose our string is $\\Omega$ = `banana`. We would start by appending the sentinel at the end to get $\\Omega'$ = `banana$`. Then we would generate all the 7 different suffixes of the string in linear time by doing a single scan over the string. This would give us the table on the left below:\n\n$$\n\\begin{array}{lr}\n\\hline \\text{start index} & \\text{suffix} \\\\ \\hline\n0 & \\text{banana}\\$ \\\\\n1 & \\text{anana}\\$ \\\\\n2 & \\text{nana}\\$ \\\\\n3 & \\text{ana}\\$ \\\\\n4 & \\text{na}\\$ \\\\\n5 & \\text{a}\\$ \\\\\n6 & \\$ \\\\\n\\hline\n\\end{array}\n\\quad\\quad\n\\begin{array}{lr}\n\\hline \\text{start index} & \\text{suffix} \\\\ \\hline\n6 & \\$ \\\\\n5 & \\text{a}\\$ \\\\\n3 & \\text{ana}\\$ \\\\\n1 & \\text{anana}\\$ \\\\\n0 & \\text{banana}\\$ \\\\\n4 & \\text{na}\\$ \\\\\n2 & \\text{nana}\\$ \\\\\n\\hline\n\\end{array}\n$$\n\nWe would then sort these suffixes to get the table on the right. The start index column of that table is the suffix array. Right off the bat, we can notice a few salient features about the suffix array. These features are the source of its utility. First, notice that all suffixes that start with the same character occupy a contiguous slice in the array. In fact, all suffixes that begin with the same prefix are all next to one another.\n\nWe implement this scheme below.\n\n```rust\n//! We will be working with indexes to different arrays a lot. Having many\n//! raw indexes flying around increases the cognitive load as it requires\n//! one to be super vigilant not to use an index of one array in another different array.\n//! To solve this problem, we use the `NewType Index Pattern`. By giving each index\n//! a concrete type, we offload the cognitive load to the compiler.\n\n/// This is an index into the underlying string\n#[derive(Debug, PartialEq, Eq, Hash, Clone)]\npub struct SuffixIndex(usize);\n\n/// To make things even more ergonomic, we implement the `Index` trait to allow\n/// us to use our new type without retrieving the wrapped index. For now,\n/// We assume that our string will be a collection of bytes. That is of course\n/// the case for the ascii alphabet\nimpl std::ops::Index\u003CSuffixIndex> for [u8] {\n    type Output = u8;\n    fn index(&self, index: SuffixIndex) -> &Self::Output {\n        &self[index.0]\n    }\n}\n\npub struct SuffixArray\u003C'a> {\n    /// The string over which we are building this suffix array\n    underlying: &'a str,\n\n    /// The suffix array is simply all the suffixes of the\n    /// underlying string in sorted order\n    suffix_array: Vec\u003CSuffixIndex>,\n}\n\n/// This is an index into the suffix array\n#[derive(Debug, PartialEq, Eq, Hash, Clone, Ord, PartialOrd)]\npub struct SuffixArrayIndex(usize);\n\n/// This allows us to easily retrieve the suffix strings by using bracket\n/// notation.\nimpl \u003C'a> std::ops::Index\u003CSuffixArrayIndex> for SuffixArray\u003C'a>  {\n    type Output = str;\n    fn index(&self, index: SuffixArrayIndex) -> &Self::Output {\n        let suffix_idx = &self.suffix_array[index.0];\n        &self.underlying[suffix_idx.0..]\n    }\n}\n```\n\nWith these abstractions in place, we can go ahead and implement our naive SACA.\n\n```rust\nimpl\u003C'a> SuffixArray\u003C'a> {\n    /// Construct the suffix array by sorting. This has worst case performance\n    /// of O(n log n)\n    pub fn make_sa_by_sorting(s: &'a str) -> Self {\n        let mut suffixes = vec![];\n        for i in 0..s.len() {\n            suffixes.push(&s[i..]);\n        }\n        suffixes.sort();\n        let mut suffix_array = vec![];\n        for suffix in suffixes {\n            let cur = SuffixIndex(s.len() - suffix.len());\n            suffix_array.push(cur);\n        }\n        Self {\n            underlying: s,\n            suffix_array,\n        }\n    }\n}\n```\n\nNow that we have our suffix array, how can we use it to do substring search? In particular, suppose that we build a suffix array for the string $\\Omega, |\\Omega| = m$ and we want to find all places that another query string $\\alpha, |\\alpha| = n$ occurs in the first string. How fast can we do this? One approach would be to use two binary searches to find the left and right region in the suffix array where our query string occurs. We'll have to make at least $\\lg m$ comparisons. To fully characterize the runtime, we need to figure out how long each comparison takes. If each lexicographic comparison is implemented by linear scanning, then the runtime to find the boundaries will be $O(n \\lg m)$. Do note that it is possible to implement string comparisons in near constant time using word-level parallelism techniques that are at the heart of [specialized containers for integers](https://github.com/jlikhuva/blog/blob/main/posts/integer.md). Once we know the boundaries of our region of interest, we can simply retrieve the matching locations by doing a linear scan. If we have $z$ matches, the total runtime for this scheme becomes $O(n \\lg m + z)$. We leave the implementation details as an exercise to the reader.\n\n## The LCP Array: Kasai's Procedure\n\nSuffix arrays are fairly useful on their own. However, they become even more useful when coupled with information about the lengths of common prefixes. In particular, the time to do substring matching is greatly reduced if we know the length of the longest common prefixes between all pairs of adjacent suffixes in the suffix array. How can we get that information? We explore that here.\n\n### A Naive Solution\n\nGiven a suffix array, the most straightforward way to construct an LCP array is to scan over the elements of the array, calculating the lengths of common prefixes for all adjacent suffixes. We implement that below.\n\n```rust\n/// The length of the longest common prefix between the\n/// suffixes that start at `left` and `right`. These\n/// suffixes are adjacent to each other in the suffix array\n#[derive(Debug)]\npub struct LCPHeight {\n    left: SuffixIndex,\n    right: SuffixIndex,\n    height: usize,\n}\n\nimpl\u003C'a> SuffixArray\u003C'a> {\n    /// Retrieve the index of the suffix stored at this location\n    /// in the suffix array. Put another way, we retrieve the id\n    /// of the (idx + 1) smallest suffix in the string\n    pub fn get_suffix_idx_at(&self, idx: usize) -> SuffixIndex {\n        // These clones are quite cheap\n        self.suffix_array[idx].clone()\n    }\n    // Add a helper to get length for Vec::with_capacity\n    pub fn len(&self) -> usize {\n        self.suffix_array.len()\n    }\n}\n\n/// Calculate the length of the longest common prefix\n/// between the two string slices in linear time\nfn calculate_lcp_len(left: &str, right: &str) -> usize {\n    let mut len = 0;\n    for (l, r) in left.as_bytes().iter().zip(right.as_bytes()) {\n        if l != r {\n            break;\n        }\n        len += 1;\n    }\n    len\n}\n\n/// The naive procedure described in the preceding section\nfn make_lcp_by_scanning(sa: &SuffixArray) -> Vec\u003CLCPHeight> {\n    let mut lcp_len_array = Vec::with_capacity(sa.len());\n    for i in 1..sa.len() {\n        let prev_sa_idx = SuffixArrayIndex(i - 1);\n        let cur_sa_idx = SuffixArrayIndex(i);\n        let lcp_len = calculate_lcp_len(&sa[prev_sa_idx], &sa[cur_sa_idx]);\n        lcp_len_array.push(LCPHeight {\n            left: sa.get_suffix_idx_at(i - 1),\n            right: sa.get_suffix_idx_at(i),\n            height: lcp_len,\n        });\n    }\n    lcp_len_array\n}\n```\n\nHow fast is this procedure? Well, clearly it takes at least $O(n)$. To get a tighter bound, we need to investigate the worst case behavior of the inner loop that calculates the `LCP` between two suffixes. Suppose that the two strings are identical except that one is one character shorter than the other. In that case, the inner loop will iterate $n-1$ times. This means that the runtime of this procedure is $O(n^2)$. This is bad. Do note that the example used is not a degenerate case, it is quite likely to occur when dealing with really long strings (for example 3 billion characters) from a really small alphabet (for instance $\\Sigma = 4$). We need a faster method.\n\n### [Kasai's Procedure](http://web.cs.iastate.edu/~cs548/references/linear_lcp.pdf)\n\nThe main reason why the naive solution is sub-optimal is the inner loop. If we could somehow reduce the time needed to compute `LCP` values, we could markedly improve the overall runtime. The first thing to observe is that when implementing the naïve procedure, we iterated over the suffix array — not the underlying string. Because of this, we are unable to exploit the fact that the only difference between two suffixes $S_i$ and $S_{i + 1}$ that are adjacent to each other *in the string* is that one has one more character at the start. Suppose we have already found the `lcp` length between $S_i$ and the $S_k$, a suffix adjacent to it *in the suffix array*, to be $h > 1$. How can we use this information to calculate the lcp length between $S_{i+1}$ and $S_j$, the suffix adjacent to it in the suffix array? The key insight stems from observing that if we delete the first character from $S_i$ we get $S_{i+1}$, and since $h > 1$ deleting that character from $S_j$ yields another suffix that is adjacent to $S_{i+1}$ with overlap in at least $h-1$ location. Therefore, to calculate the `lcp` between the shorter suffixes, we do not need to compare the first $h-1$ suffixes for we already know that they are the same. This effectively reduces the number of times the inner loop iterates and results in a linear time solution. See the linked paper for a proof of correctness and runtime analysis. We implement this scheme below.\n\n```rust\nuse std::collections::HashMap;\n\nimpl From\u003C(SuffixIndex, SuffixIndex, usize)> for LCPHeight {\n    fn from((l, r, h): (SuffixIndex, SuffixIndex, usize)) -> Self {\n        LCPHeight {\n            left: l,\n            right: r,\n            height: h,\n        }\n    }\n}\n/// Computes the lcp array in O(n) using Kasai's algorithm. This procedure assumes that\n/// the sentinel character has been appended onto `s`.\nfn make_lcp_by_kasai(s: &str, sa: &SuffixArray) -> Vec\u003CLCPHeight> {\n    let n = s.len();\n    if n == 0 { return Vec::new(); }\n\n    let s_ascii = s.as_bytes();\n    let mut lcp_values = vec![0; n];\n\n    // rank_array[i] stores the rank of suffix s[i..] in the suffix array.\n    let mut rank_array = vec![0; n];\n    for r in 0..n {\n        rank_array[sa.get_suffix_idx_at(r).0] = r;\n    }\n\n    let mut h = 0;\n    for i in 0..n {\n        let rank_of_si = rank_array[i];\n        if rank_of_si > 0 {\n            let j = sa.get_suffix_idx_at(rank_of_si - 1).0;\n            while i + h \u003C n && j + h \u003C n && s_ascii[i + h] == s_ascii[j + h] {\n                h += 1;\n            }\n            lcp_values[rank_of_si] = h;\n            if h > 0 {\n                h -= 1;\n            }\n        }\n    }\n\n    let mut lcp_height_array = Vec::with_capacity(if n > 0 { n - 1 } else { 0 });\n    for rank in 1..n {\n        lcp_height_array.push(LCPHeight {\n            left: sa.get_suffix_idx_at(rank-1),\n            right: sa.get_suffix_idx_at(rank),\n            height: lcp_values[rank]\n        });\n    }\n    lcp_height_array\n}\n```\n\n## The Suffix Array: A Linear Time Solution [WIP]\n\nIn the first section, we implemented a suffix array construction algorithm (SACA) that worked by sorting the suffixes. During that discussion, we noted that the runtime of that scheme is lower bounded by the time it takes to sort the suffixes. For long sequences, this time can be quite large. For example, we may want to build a suffix array of the human genome approx: 3 billion characters. Can we do better? Can we shave off a log factor? Yes. Yes we can. We won't use the method of four russians though.\n\n### [SA-IS: A suffix array via Induced Sorting](https://ieeexplore.ieee.org/document/4976463)\n\nWhat is `induced sorting?` and how does it differ from normal sorting? The word `induce` in the title of this procedure refers to inductive reasoning or, more plainly, inference. `induced sorting` is thus sorting by inference. Note that I'm using the term `inference` in its natural language sense, not its statistical sense. As we shall see, in induced sorting, we are able to infer the order of certain suffixes once we know the order of some specific suffixes. This means that we can sort without comparisons and can thus beat the $n \\lg n$ lower bound that hamstrung the naive SACA method.\n\n#### Foundational Concepts\n\nBelow, we briefly discuss some key ideas that we need in order to fully understand the SA-IS procedure.\n\n**L-Type & S-Type Suffixes:** A suffix starting at some position $k$ in some text $T$ is an `S-type` suffix if: $T[k] \u003C T[k + 1]$, OR $T[k]==T[K + 1]$ and $k + 1$ is an S-type suffix, OR $T[k]$ is the sentinel character. Similarly, a suffix starting at some position $k$ in some text $T$ is an `L-type` suffix if $T[k] > T[k + 1]$, OR $T[k] == T[K + 1]$ and $k + 1$ is an L-type suffix.\n\n**LMS Suffixes and Substrings:** An `S` type suffix is said to be a Left Most S-suffix (LMS suffix for short) if it is an `S` type suffix that has an `L` type suffix as its left neighbor. The sentinel `$` is an `LMS` suffix by definition. An `LMS substring` is a contiguous slice of the underlying string that starts at the starting index of some `LMS` suffix and runs up to the start of the next, closest `LMS` suffix.\n\nWhat's the purpose of all these concepts? Well, SA-IS is based on two key ideas. The first one is that if we know the locations of the `LMS` suffixes in the suffix array, then we can infer the location of all the other suffixes (induced sorting). The second one is divide and conquer. Since SA-IS is a divide and conquer method it needs a way of reducing the problem space. This is called `substring renaming` in the literature. Since `LMS` suffixes are sparsely distributed in a string, SA-IS leverages the substrings that they produce (`LMS Substrings`) to reduce the problem size. We shall discuss how `substring renaming` is done later on. For now, let us introduce abstractions that allow us to work with the concepts we have seen so far.\n\n```rust\n#[derive(Debug, PartialEq, Eq, Clone)]\nenum SuffixType {\n    S(bool), // true if LMS\n    L,\n}\n\n#[derive(Debug)]\npub struct Suffix\u003C'a> {\n    start: SuffixIndex,\n    suffix_type: SuffixType,\n    underlying: &'a str,\n}\n\nimpl\u003C'a> Suffix\u003C'a> {\n    pub fn is_lms(&self) -> bool {\n        match self.suffix_type {\n            SuffixType::L => false,\n            SuffixType::S(lms) => lms,\n        }\n    }\n}\n\nimpl\u003C'a> From\u003C(SuffixIndex, SuffixType, &'a str)> for Suffix\u003C'a> {\n    fn from((start, suffix_type, underlying): (SuffixIndex, SuffixType, &'a str)) -> Self {\n        Suffix {\n            start,\n            suffix_type,\n            underlying,\n        }\n    }\n}\n\nimpl\u003C'a> SuffixArray\u003C'a> {\n    fn create_suffixes(underlying: &'a str) -> (Vec\u003CSuffix\u003C'a>>, Vec\u003CSuffixIndex>) {\n        let s_len = underlying.len();\n        if s_len == 0 { return (Vec::new(), Vec::new()); }\n\n        let mut tags = vec![SuffixType::S(false); s_len];\n        let s_ascii = underlying.as_bytes();\n\n        if s_len > 1 {\n            for i in (0..s_len - 1).rev() {\n                if s_ascii[i] > s_ascii[i + 1] {\n                    tags[i] = SuffixType::L;\n                } else if s_ascii[i] == s_ascii[i + 1] {\n                    if tags[i + 1] == SuffixType::L {\n                        tags[i] = SuffixType::L;\n                    } else {\n                        tags[i] = SuffixType::S(false);\n                    }\n                } else {\n                    tags[i] = SuffixType::S(false);\n                }\n            }\n        }\n\n        if s_len > 0 {\n            if let SuffixType::S(ref mut is_lms) = tags[s_len-1] {\n                *is_lms = true;\n            }\n        }\n\n        if s_len > 1 {\n            for i in (0..s_len - 1).rev() {\n                if i > 0 {\n                    if tags[i-1] == SuffixType::L {\n                        if let SuffixType::S(ref mut is_lms) = tags[i] {\n                            *is_lms = true;\n                        }\n                    }\n                }\n            }\n        }\n\n        let mut final_lms_locations = Vec::new();\n        for i in 0..s_len {\n            if let SuffixType::S(true) = tags[i] {\n                final_lms_locations.push(SuffixIndex(i));\n            }\n        }\n\n        let mut suffixes_vec = Vec::with_capacity(s_len);\n        for (i, tag) in tags.into_iter().enumerate() {\n            suffixes_vec.push(Suffix::from((SuffixIndex(i), tag, underlying)));\n        }\n        (suffixes_vec, final_lms_locations)\n    }\n}\n```\n\n**Buckets:** A bucket is a contiguous region of the suffix array where all suffixes begin with the same character. That starting character serves as the label for that bucket. Buckets are important in SA-IS because, as we'll soon see, we use them for induced sorting. Specifically, by placing `LMS` suffixes in their buckets at the right locations, we are able to infer the appropriate locations of the other suffixes in those buckets. Below we introduce the abstraction for a bucket.\n\n```rust\n/// The first character of the suffixes in a bucket\n/// uniquely identifies that bucket\n#[derive(Eq, PartialEq, Hash, Debug, Clone, Copy)]\npub struct BucketId\u003CT>(T);\n\n#[derive(Debug, Clone)]\npub struct Bucket {\n    start: SuffixArrayIndex,\n    end: SuffixArrayIndex,\n    offset_from_start: usize,\n    offset_from_end: usize,\n    lms_offset_from_end: usize,\n}\n\nimpl\u003C'a> Suffix\u003C'a> {\n    pub fn get_bucket(&self) -> BucketId\u003Cu8> {\n        let first_char_byte = self.underlying.as_bytes().get(self.start.0);\n        debug_assert!(first_char_byte.is_some(), \"Suffix start index out of bounds\");\n        BucketId(*first_char_byte.unwrap())\n    }\n}\n```\n\n**The Alphabet $\\Sigma$:** An alphabet, simply put, is the ordered unique list of all the characters that can ever appear in our strings. For instance, if our problem domain is genomics, then our alphabet could be `{A, T, U, C, G}`. If it is proteomics, the alphabet could be the 20 amino acids. For our case, our alphabet is the `256` ascii characters. To create a bucket, we need to know where it starts and where it ends. For each unique character in the underlying string, we can obtain this information by leveraging the associated alphabet to implement a modified version of counting sort. In particular, we start by keeping a count of how many times each character appears in the underlying string — just as we do in counting sort. We then scan across this array of counts using the count information to generate our buckets. We implement this functionality below.\n\n```rust\npub struct AlphabetCounter([usize; 1 \u003C\u003C 8]);\nimpl AlphabetCounter {\n    pub fn from_ascii_str(s: &str) -> Self {\n        let mut alphabet = [0_usize; 1 \u003C\u003C 8];\n        for byte in s.as_bytes() {\n            alphabet[*byte as usize] += 1;\n        }\n        Self(alphabet)\n    }\n\n    pub fn create_buckets(&self) -> HashMap\u003CBucketId\u003Cu8>, Bucket> {\n        let mut buckets = HashMap::new();\n        let mut start_location = 0;\n        let alphabet_counter = self.0;\n        for i in 0..alphabet_counter.len() {\n            if alphabet_counter[i] > 0 {\n                let count = alphabet_counter[i];\n                let end_location = start_location + count - 1;\n                let bucket = Bucket {\n                    start: SuffixArrayIndex(start_location),\n                    end: SuffixArrayIndex(end_location),\n                    offset_from_end: 0,\n                    offset_from_start: 0,\n                    lms_offset_from_end: 0,\n                };\n                buckets.insert(BucketId(i as u8), bucket);\n                start_location = end_location + 1;\n            }\n        }\n        buckets\n    }\n}\n```\n\n**Induced Sorting Part One:** Once we know the nature of all our suffixes and the locations of all our buckets, we can begin slotting suffixes in place. First, we place all `lms` suffixes in their buckets. Because they are `S` type suffixes, we know that they will occupy the latter portions of their respective buckets. Why is this so? Well, think about how the suffixes in a given bucket compare if we exclude the first character. Once we have all the lms suffixes in position, we proceed to place L type suffixes at their appropriate location, after which we do the same for S type suffixes. We implement this logic below.\n\n```rust\ntype Buckets = HashMap\u003CBucketId\u003Cu8>, Bucket>;\n\nimpl Bucket {\n    fn insert_stype_suffix(&mut self, suffix: &Suffix, sa: &mut [SuffixIndex]) {\n        sa[self.end.0 - self.offset_from_end] = suffix.start.clone();\n        self.offset_from_end += 1;\n    }\n\n    fn insert_lms_suffix(&mut self, suffix: &Suffix, sa: &mut [SuffixIndex]) {\n        sa[self.end.0 - self.lms_offset_from_end] = suffix.start.clone();\n        self.lms_offset_from_end += 1;\n    }\n\n    fn insert_ltype_suffix(&mut self, suffix: &Suffix, sa: &mut [SuffixIndex]) {\n        sa[self.start.0 + self.offset_from_start] = suffix.start.clone();\n        self.offset_from_start += 1;\n    }\n}\n\nimpl\u003C'a> SuffixArray\u003C'a> {\n    fn induced_sort_pass1(\n        s: &'a str,\n        suffixes_meta: &[Suffix\u003C'a>],\n        lms_indices: &[SuffixIndex],\n        buckets: &mut Buckets,\n        sa: &mut [SuffixIndex]\n    ) {\n        for item in sa.iter_mut() {\n            *item = SuffixIndex(usize::MAX);\n        }\n\n        for bucket_val in buckets.values_mut() {\n            bucket_val.offset_from_start = 0;\n            bucket_val.offset_from_end = 0;\n            bucket_val.lms_offset_from_end = 0;\n        }\n\n        // 1. Place LMS suffixes into SA from right-to-left in their buckets\n        for lms_idx_obj in lms_indices.iter().rev() {\n            let cur_lms_suffix = suffixes_meta.iter().find(|s_obj| s_obj.start == *lms_idx_obj)\n                .expect(\"LMS index must correspond to a suffix object\");\n            let bucket_id = cur_lms_suffix.get_bucket();\n            if let Some(bucket_ref) = buckets.get_mut(&bucket_id) {\n                bucket_ref.insert_lms_suffix(cur_lms_suffix, sa);\n            }\n        }\n\n        for bucket_val in buckets.values_mut() {\n            bucket_val.offset_from_start = 0;\n            bucket_val.offset_from_end = bucket_val.lms_offset_from_end;\n        }\n\n        // 2. Scan SA left-to-right. If SA[p] is L-type, place T[SA[p]-1] into its bucket head.\n        for p in 0..sa.len() {\n            let current_sa_val = &sa[p];\n            if current_sa_val.0 != usize::MAX && current_sa_val.0 > 0 {\n                let prev_suffix_start_idx = SuffixIndex(current_sa_val.0 - 1);\n                let prev_suffix_obj = suffixes_meta.iter().find(|s_obj| s_obj.start == prev_suffix_start_idx)\n                    .expect(\"Previous suffix must exist\");\n                if prev_suffix_obj.suffix_type == SuffixType::L {\n                    let bucket_id = prev_suffix_obj.get_bucket();\n                    if let Some(bucket_ref) = buckets.get_mut(&bucket_id) {\n                        bucket_ref.insert_ltype_suffix(prev_suffix_obj, sa);\n                    }\n                }\n            }\n        }\n\n        for bucket_val in buckets.values_mut() {\n            bucket_val.offset_from_end = bucket_val.lms_offset_from_end;\n        }\n\n        // 3. Scan SA right-to-left. If SA[p] is S-type, place T[SA[p]-1] into its bucket tail.\n        for p in (0..sa.len()).rev() {\n            let current_sa_val = &sa[p];\n            if current_sa_val.0 != usize::MAX && current_sa_val.0 > 0 {\n                let prev_suffix_start_idx = SuffixIndex(current_sa_val.0 - 1);\n                let prev_suffix_obj = suffixes_meta.iter().find(|s_obj| s_obj.start == prev_suffix_start_idx)\n                    .expect(\"Previous suffix must exist\");\n                if let SuffixType::S(_) = prev_suffix_obj.suffix_type {\n                    let bucket_id = prev_suffix_obj.get_bucket();\n                    if let Some(bucket_ref) = buckets.get_mut(&bucket_id) {\n                        bucket_ref.insert_stype_suffix(prev_suffix_obj, sa);\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n**Substring Renaming:** In substring renaming, we'd like to reduce the size of our input. We do so by forming a new string out of our `LMS substrings`. That is, all the characters that belong to a single substring are reduced to a single label. We implement substring renaming below.\n\n```rust\n/// WIP\n```\n\nAfter obtaining the shorter substring, we check to see if it contains any repeated labels. If it does not, then we are done, we can go ahead and create a suffix array for the reduced string. If it contains duplicates, we recurse on the reduced string and alphabet. In the next section we discuss how to compute the suffix array given the suffix array of the reduced string.\n\n**Inducing the SA from an approximate SA:**\n\n```rust\n/// WIP\n```\n\n#### The Whole Enchilada\n\n```rust\n/// WIP\n```\n\n## References\n\n1. [CS 166 Lecture 3](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/03/Small03.pdf)\n2. [CS 166 Lecture 4](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/04/Small04.pdf)\n3. [This Exposition](https://zork.net/~st/jottings/sais.html#induced-sorting-l-type-suffixes)","src/content/writing/str-indexing.mdx","ed6e9a1ee74b6993","str-indexing.mdx","streaming-svd",{"id":130,"data":132,"body":18,"filePath":136,"digest":137,"legacyId":138,"deferredRender":17},{"title":133,"date":134,"tags":135,"draft":17},"The Streaming SVD Procedure",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/writing/streaming-svd.mdx","5c4f310bb900cf47","streaming-svd.mdx","tiledb-linalg",{"id":139,"data":141,"body":18,"filePath":145,"digest":146,"legacyId":147,"deferredRender":17},{"title":142,"date":143,"tags":144,"draft":17},"Out of Core Linear Algebra using TileDB",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/writing/tiledb-linalg.mdx","36fc76fafda16bbc","tiledb-linalg.mdx","trace",{"id":148,"data":150,"body":18,"filePath":154,"digest":155,"legacyId":156,"deferredRender":17},{"title":151,"date":152,"tags":153,"draft":17},"Trace and Trace Function Estimation",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/writing/trace.mdx","fc9bdc95bb5a8445","trace.mdx","wlp",{"id":157,"data":159,"body":165,"filePath":166,"digest":167,"legacyId":168,"deferredRender":17},{"title":160,"date":161,"description":162,"tags":163,"draft":60,"citationKey":157},"Word Level Parallelism",["Date","2021-06-01T00:00:00.000Z"],"Exploring bit-level algorithms for parallel operations on packed integers, including parallel compare, tile, rank, and O(1) MSB.",[57,164,59],"bit-manipulation","import Sidenote from '../../components/Sidenote.astro';\n\n\u003CSidenote>\n  \u003Ca href=\"https://crates.io/crates/bit-parallelism\" target=\"_blank\">\n    \u003Cimg src=\"https://img.shields.io/crates/v/bit-parallelism.svg?style=for-the-badge&logo=rust\" alt=\"Crates.io\" />\n  \u003C/a>\n  \u003Ca href=\"https://docs.rs/bit-parallelism\" target=\"_blank\">\n    \u003Cimg src=\"https://img.shields.io/docsrs/bit-parallelism/latest?style=for-the-badge&logo=docs.rs\" alt=\"Documentation\" />\n  \u003C/a>\n\u003C/Sidenote>\n\nIn this note, we shall explore a few bit level algorithms that we'll need when developing some of the\nspecialized integer data structures. We begin by discussing a procedure that allows us to extract the\nfirst (the top) `k` bits of any integer in constant time. We then proceed to discuss procedures that\nallow us, in constant time, to operate on integers in parallel.\n\n## Finding the `top(k)` bits of an integer\n\nThe first procedure is quite simple. The goal is, given a number `x` and a length `k`, to extract\nthe first `k` bits of `x` in $O(1)$. A procedure that does this will be handy when implementing\nthe x-fast trie.\n\n```rust\nconst USIZE_BITS: usize = 64;\npub fn top_k_bits_of(x: usize, k: usize) -> usize {\n    assert!(k != 0);\n    let mut mask: usize = 1;\n\n    // Shift the 1 to the index that is `k`\n    // positions from the last index location.\n    // That is `k` away from 64\n    mask \u003C\u003C= USIZE_BITS - k;\n\n    // Turn that one into a zero. And all\n    // the other 63 zeros into ones. This\n    // basically introduces a hole. in the next\n    // step, we'll use this hole to trap a cascade\n    // of carries\n    mask = !mask;\n\n    // I think this is the most interesting/entertaining part.\n    // Adding a one triggers a cascade of carries that flip all\n    // the bits (all ones) before the location of the zero from above into\n    // zeros. The cascade stops when they reach the zero from\n    // above. Since it is a zero, adding a 1 does not trigger a carry\n    //\n    // In the end, we have a mask where the top k bits are ones\n    mask += 1;\n\n    // This is straightforward\n    x & mask\n}\n```\n\nYou can play around with the code so far [in the playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=186242f8f5e9267430438fcb3119606c).\n\n---\n\n## Background\n\nArithmetic and logical operations take, for all intents and purposes, constant time. Such operations\noperate on whole words. (A word is the size of a single memory segment. In this exposition, we assume\na word size width of `64`. For a more in-depth discussion of computer memory, refer to\n[this note](https://akkadia.org/drepper/cpumemory.pdf)). For instance, it takes constant time to add\ntwo `64` bit numbers. The central idea of the methods we're about to discuss is this: If you have a\nbunch of small integers — each smaller than sixty four bits, e.g. a bunch of bytes, we can pack many\nof them into a single sixty four bit integer. Then, we can operate on that packed integer as if it\nwere a single number. For example, we can fit 8 byte-sized numbers in a single word. By operating on\nthe packed integer, we are in effect operating on 8 different integers in parallel. This is what we\ncall `world level parallelism`. Of course there are intricate details that have been elided in this\nbroad description. In the next sections, we take a detailed look at those details as we flesh out the\ndifferent parallel operations.\n\n## Motivation: B-Trees\n\nSuppose we wish to maintain a set of small sized integers in a B-Tree. And suppose too that we wish\nto take advantage of the fact that we can fit many of these integers in a single, larger integer. How\nwould we go about designing such a B-Tree?\n\nRecall that a B-Tree of order `b` is a multiway search tree in which each node is a bucket that must\ncontain between `b - 1` and `2b - 1` keys. Furthermore, each node has one more child than the number\nof keys it contains. That is, each node must have between `b` and `2b` child nodes. Operations on\nB-Trees rely on one key operation: `node.rank(x)`. This operation searches through the keys of a\nsingle node (which are sorted) and either returns the location of `x` in the bucket, or the index of\nthe child we need to descend into in order to complete the operation at hand. In run of the mill\nB-Trees, `node.rank(x)` is implemented using binary search and thus takes $O(\\lg b)$. If we know\nthat our keys are small integers, can we perform `node.rank(x)` in $O(1)$? It turns out we can. In\nthe rest of this section, we start by building up the subroutines that we'll need in order to\nimplement $O(1)$ `node.rank(x)`. After that, we'll present the $O(1)$ `node.rank(x)`\nprocedure.\n\n## Parallel Compare\n\nThe `node.rank(x)` operation depends on an even more basic operation: `compare(x, y)`. This\noperation simply tells us if $x \\ge y$. Suppose `x` and `y` are `7-bits` wide (but stored in an 8\nbyte integer such that the final bit is unoccupied), how could we implement `compare(x, y)`? Well,\nwe could do it the `C-`way — by subtraction. However, instead of taking the usual route where we\ncalculate $z = x - y$ and if $z$ is negative we know that $x \u003C y$ otherwise, we conclude that\n$x \\ge y$, we'll adopt a much cooler approach. In particular, we'll first set the 8th bit of `y`,\nthe 7-bit number we are comparing against to `1`. We'll also set the 8th bit of `x` to 0. For\nexample, suppose $x = 0000111$ and $y = 1100001$. Below, we show the effect of setting the\n8th bit.\n\n\u003Cdiv align=\"center\">\n\n$$\n\\begin{aligned}\ny = 1100001 &\\rightarrow \\textcolor{red}{1}1100001 \\\\\nx = 0000111 &\\rightarrow \\textcolor{cyan}{0}0000111\n\\end{aligned}\n$$\n\n\u003C/div>\n\nNow, if we compute $y - x$, how will that 8th bit behave? Below, we show the result of this\nsubtraction.\n\n\u003Cdiv align=\"center\">\n\n$$\n\\begin{aligned}\ny &= \\textcolor{red}{1}1100001 \\\\\nx &= \\textcolor{cyan}{0}0000111 \\\\\n\\hline\ny - x &= \\textcolor{green}{1}1011010\n\\end{aligned}\n$$\n\n\u003C/div>\n\nAs shown above, when $y \\ge x$ the sentinel bit in the result is turned on. Had $x$ been larger\nthan $y$, that bit would have been turned off. Why does this happen? Well, it is a direct consequence\nof the borrowing that we have to do if the number we are subtracting from, $y$ in this case, is\nsmaller than the amount we are subtracting ($x$). When we borrow, we'll eventually have to dip into\nthe sentinel bit thus turning it off. To summarize, to compare two 7-bit integers, we set the 8th\nbit to `0` in one of the numbers and to `1` in the other. We then subtract the resultant number and\ncheck the value of the `8th` bit.\n\nSo far, we've been talking about how to compare $x$ with a single `7bit` number. However, for our\nprocedure to be useful as a subroutine in computing `node.rank(x)`, we need to compare $x$ with $b$\n7-bit numbers. Note that we should choose $b$ such that it fits in 64 bits, so we choose $b=7$.\nThat is, a single node holds 7, 7-bit numbers, each represented using 8 bits. If those seven numbers\nare organized such that each number has an associated sentinel bit that is set to `0`, we can compare\n$x$ to all of them by comparing the entire word with a number formed by tiling $x$ 7 times. Here is\nan example of seven small integers packed in a single machine word.\n\n\u003Cdiv align=\"center\">\n\n$$\n\\begin{aligned}\n\\text{\\_}11101110\\text{\\_}10101110\\text{\\_}11111000\\text{\\_}11001101\\text{\\_}10101111\\text{\\_}10001101\\text{\\_}11110111\\text{\\_}11100001 \\\\\n\\text{\\_}01010110\\text{\\_}00111110\\text{\\_}00111110\\text{\\_}01000011\\text{\\_}00011011\\text{\\_}00101111\\text{\\_}00100011\\text{\\_}01111010\n\\end{aligned}\n$$\n\n\u003C/div>\n\nTo recap, here is how we'd compare two collections of small integers in parallel:\n\n- First, we pack the first collection, $x_1, x_2, \\ldots x_n$ into single machine word `X`,\n  separating each integer with a sentinel bit set to `0`.\n- Then we also pack the second collection of small integers, $y_1, y_2, \\ldots y_n$, into another\n  machine word `Y`. However, this time, we separate each integer with a sentinel bit that is set\n  to `1`.\n- Then we calculate $X - Y$. The bit preceding $x_i, y_i$ is `1` if $x_i \\ge y_i$.\n\nThe scheme above leaves two key questions unanswered. First, how do we perform the first and second\nsteps, packing small integers into a single machine word? Can that be done in $O(1)$? Second, after\nwe have done the subtraction, how do we read off the sentinel bits in order to get the actual\ncomparison information that we need? Below, we address the first question. We shall come back to the\nsecond question shortly afterwards.\n\n## Parallel Tile\n\nWhen introducing this note, we asked how we could store a bunch of small numbers in a B-Tree, while\ntaking advantage of the fact that our integers can be packed in a machine word. We then realized that\ndoing so would require us to find a routine that can find the rank of a number `x` in a given node\nin constant time. Thus far, we have found a rudimentary primitive, parallel compare, that allows us\nto compare many small integers in parallel. How would we use this primitive in our specialized B-Tree?\n\nTo use our primitive, we shall represent each node as a single machine word. That is, the bucket for\neach node will simply be a `64-bit` number. When storing a small integer into this bucket, we shall\nalso prepend it with a `0`. That is, the keys in our bucket will be separated by sentinel bits set\nto `0`. Below, we introduce the abstraction for a single node in our B-Tree.\n\n```rust\n/// The abstraction for a single node in our b-tree\n/// that is specialized for holding small integers\n/// that can be packed into a single machine word\n#[derive(Debug, Default)]\npub struct SardineCan {\n    /// The actual storage container\n    buckets: u64,\n\n    /// The count of items in this node.\n    count: u8,\n}\n\nimpl SardineCan {\n    /// Procedure to store a single small integer in a given node\n    /// Note that we do not handle the case where a can could be full.\n    /// We ignore that because, ideally, this data structure would be part\n    /// of a larger B-Tree implementation that would take care of such details\n    pub fn add(&mut self, mut x: u8) {\n        // Add the sentinel bit. It is set to 0\n        x &= 0b0111_1111;\n\n        // Make space in the bucket for the new item\n        self.buckets \u003C\u003C= 8;\n\n        // Add the new item into the bucket\n        self.buckets  |= x as u64;\n\n        // Increment the count of items\n        self.count += 1;\n    }\n}\n```\n\nWe've seen how we're going to store our small integers in a single machine word. However, we've yet\nto fully answer the question of how to pack the integers though. Note that we only know how to pack\nwhen adding. How about when we have a single integer and we wish to check if it's in the can? That\nis, How to perform a lookup in a sardine can? To perform a lookup for the key `k`, we begin by\nreplicating `k` seven times to form a number that is as wide as our can. Furthermore, we separate\neach small number in the packed integer using a sentinel bit that is set to 1. The act of forming\nthe replicated number is known as tiling. How can we tile the query `k` in $O(1)$? We are going to\nuse multiplication. The key insight comes from the observation that in base 10, multiplying a number\nby a multiplier that contains ones at specific locations has the effect of replicating that number.\nFor instance, $8 \\times 11 = 88$, $8 \\times 101 = 808$, $8 \\times 1001001001001 = 8008008008008$,\n$125 \\times 10000010001 = 1250001250125$. We'd like to use this intuition to tile our query `k`.\nBefore we do that, let's examine the multiplications a bit more. Why does $8 \\times 111$ produce\n$888$? It does so because:\n\n\u003Cdiv align=\"center\">\n\n$$\n\\begin{aligned}\n888 &= (8 \\times 10^2) + (8 \\times 10^1) + (8 \\times 10^0)\\\\\n&= (8 \\ll_{10} 2 ) + (8 \\ll_{10} 1) + (8 \\ll_{10} 0) \\\\\n&= (800 + 080 + 008)\n\\end{aligned}\n$$\n\n\u003C/div>\n\nTherefore, all we need to tile `k` is to find a multiplier that will have a similar effect. The\nlogic below shows one such multiplier. It also includes an additional step where we set the sentinel\nspacer bits to 1.\n\n```rust\nimpl SardineCan {\n    pub fn parallel_tile(x: u8) -> u64 {\n        // This carefully chosen multiplier will have the desired effect of replicating `x`\n        // seven times, interspersing each instance of `x` with a 0\n        let multiplier: u64 = 0b10000000_10000000_10000000_10000000_10000000_10000000_100000001;\n\n        // Produce the provisional tiled number. We still need to set its\n        // sentinel bits to 1\n        let tiled_x = x as u64 * multiplier;\n\n        // The bitmask to turn on  the sentinel bits\n        let sentinel_mask: u64 = 0b10000000_10000000_10000000_10000000_10000000_10000000_1000000010000000;\n\n        // Set the sentinel bits to 1 and return the tiled number\n        tiled_x | sentinel_mask\n    }\n}\n```\n\nAt this point, we are able to add small integers into a single node in our hypothetical B-Tree. We\nare also able to replicate a query `k` to form a number that is as large as the word size we're\nusing. This means that we can do a parallel comparison via subtraction. After we perform the\nsubtraction — subtracting the value stored in the can from our tiled key —, we'll be left with a\nnumber whose sentinel bits indicate whether the key is less than or equal to the associated small\ninteger. To be more precise, the sentinel bit for a small integer in the difference will be 1 if the\nassociated small integer is $\\le$ our key. The next thing we'd like to do is count how many values\nare $\\le$ our key. This is the rank of our key. We need to come up with a procedure that does this\nin $O(1)$. We'll explore that in the next section. Before moving on, feel free to check out the code\nthus far [here](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=7688364fc546c19abc25f1a6264142dc).\n\n## Parallel Rank\n\nAt this point, we can store tiny numbers in a given node and, when a key `x` comes in, we can tile\nit and perform the first step of a parallel comparison operation. To finish out the `node.rank(x)`\noperation, we need to discuss how to count up the number of items smaller than or equal to our key.\nTo find this number, we simply need to count how many of the sentinel bits in the difference are set\nto `1`. We want to do this in $O(1)$. One option is to use a series of shifts to align all the\nsentinel bits, then adding them up. To perform the shifting in a single operation, we use the same\nidea we used when implementing `parallel_tile` — we multiply with a carefully chosen spreader. While\nthis approach works well, we adopt a much simpler approach. After we have isolated the sentinel bits\nin the difference by performing a bitwise and with an appropriate mask, we simply call a rust built-in\nprocedure to count the number of ones.\n\n```rust\nimpl SardineCan {\n    /// Calculate how many items in this can are less than or\n    /// equal to `x`\n    pub fn parallel_rank(&self, x: u8) -> u8 {\n        // Perform the parallel comparison\n        let mut difference = Self::parallel_tile(x) - self.buckets;\n\n        // Ultimately, we're only interested in whether the spacer sentinel bits\n        // are turned on or off. In particular, we just need to know how many are\n        // turned on. Here we use the mask from `parallel_tile` to isolate them\n        let sentinel_mask: u64 = 0b10000000_10000000_10000000_10000000_10000000_10000000_1000000010000000;\n        difference &= sentinel_mask;\n\n        // There's an alternative method of counting up how many spacer bits are set to 1.\n        // That method involves using a well chosen multiplier. To check it out look in\n        // at the `parallel_count` method below\n        difference.count_ones() as u8\n    }\n\n    /// Counts up how many of the sentinel bits of `difference` are turned on.\n    /// This could be used instead of using the builtin function `count_ones`.\n    fn parallel_count(difference: u64) -> u8 {\n        let stacker = 0b10000000_10000000_10000000_10000000_10000000_10000000_100000001u64;\n        let mut stacked = difference as u128 * stacker as u128;\n        stacked >>= 63;\n        stacked &= 0b111;\n        stacked as u8\n    }\n}\n```\n\nAt this point, we have implemented all the (new) routines that would be needed to implement a B-Tree\nthat uses a `SardineCan` as its node. If we were to implement such a tree, what would be the runtime\nof each dictionary operation? In a normal B-Tree, all these operations take on the order of\n$\\mathcal{O}(\\lg_b n \\cdot \\lg_2 b) = O(\\lg_2 n)$ (Note, however, that when analyzing B-tree, we\noften only count the number of blocks that have to be read into memory and consider the time to\nsearch within a single block to be near constant). In a B-Tree that packs its keys using techniques\ndiscussed above, its runtime would be $\\mathcal{O}(\\lg_b n \\cdot 1) = O(\\lg_b n)$. Even more\ndramatically, its space usage will be $\\Theta(\\frac{n}{b})$. Remember, we defined `b` as the number\nof small integers we could pack in a single machine word — `8` in our case.\n\n[Here's a link](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=9ea0b3715a46d728d9a815e6ce3d9597) to all the methods we've implemented so far.\n\n---\n\n## $O(1)$ Most Significant Bit\n\nWhen we talk of the most significant bit of a number, we're often referring to the 0-indexed location\nof the highest bit set. Note that this is a more general problem than simply finding the number that\nwould be formed if only the `msb` were set. For instance, `MSB(010010001)` is `7` and not `128`.\n\nThe simplest method for finding this index is by doing a linear scan over the bits of the number in\nquestion while keeping a count of the number of bits seen thus far. This scheme runs in $O(\\lg n)$\nwhere $n$ is the highest number our function may operate on.\n\n```rust\n/// A procedure for finding the index of the most significant\n/// bit in time linear to the number of bits used\n/// to represent the value.\nfn get_msb_idx_of(query: u64) -> u8 {\n    for i in (0..64).rev() {\n        if query & (1 \u003C\u003C i) != 0 {\n            return i;\n        }\n    }\n    panic!(\"MSB(0) is undefined\")\n}\n```\n\nWe can improve upon the linear scanning procedure using bit level binary search. This brings down\nthe running time to $O(\\lg \\lg n)$. Often, however, when we know that we'll be doing many `msb`\nqueries, we use a lookup table to compute this value. This is the solution we adopted when discussing\nsparse tables in the context of [the range min query problem](./rmq#querying-the-sparse-table). Using\nthat method, we're able to locate the index of the highest bit set in constant $O(1)$ time, albeit\nwith an added preprocessing step to build the lookup table. The question we seek to answer in this\nsection is, can we locate the index of the most significant bit in constant time without using a\nlookup table?\n\n### The MSB(x)–Rank(x) Equivalence\n\nAs a reminder, `MSB(x)` is the largest value of $k$ such that $2^k \\leq x$. Remember also the\ndefinition of `Rank(x)`: it is the number of items, in some underlying ordered collection, that are\nless than or equal to $x$. Immediately, we can see how eerily similar these two definitions are. In\nfact, we can redefine `MSB(x)` in terms of `Rank(x)` by choosing an appropriate underlying\ncollection. If the underlying collection is a list of all the powers of two,\n$\\langle 2^0, 2^1, 2^2, \\ldots 2^{62}, 2^{63} \\rangle$, then `MSB(x)` is the same as `Rank(x)` in\nthat array.\n\n### Parallel Pack\n\nAt this point, we're one step closer to the solution. We've redefined `MSB` as `Rank` and we have a\nmethod for solving `Rank` in constant time — `parallel_rank`. That procedure, however, assumes that\nthe underlying array can be packed in a single machine word. It also assumes that our query $x$ is\nas wide as our packed underlying array. Clearly, we cannot fit `64` numbers in a single machine\nword. However, if we can somehow reduce the size of our query, then, maybe, we can reduce the size\nof the needed underlying array. To do that, we'll borrow a technique that we saw extensively while\ndiscussing [the median of medians method](https://github.com/jlikhuva/blog/blob/main/posts/rmq.md#the-method-of-four-russians):\nBlock Decomposition.\n\nWe'll decompose the bits of our query into blocks of `8` bits. This means that the underlying array\nof powers of two will only have `8` elements down from `64`. This still won't fit in a single machine\nword — remember that when discussing `parallel_rank`, we assumed that our numbers had only `7 bits`.\nNevertheless, we can still pack our 8-bit numbers in two machine words (`u128` instead of `u64`)\nwithout affecting the runtime of `parallel_rank`. One final piece of the puzzle still remains. How\nwill we know which block to focus on? We need some sort of routing information.\n\nFor routing, we'll keep a secondary 8-bit array with one entry for each block. The `k-th` entry of\nthis array contains a `1` if and only if the `k-th` block of our query has a numeric value greater\nthan zero (that is, the block contains a `1` anywhere within it). Note that the size of this\nsecondary bit array is small enough that we can pack it into a few machine words. This means that we\ncan leverage our previous methods when operating on it.\n\nTo recapitulate: To answer `MSB(x)`, we'll\n\n1. Decompose the bit representation of $x$ into blocks of 8 bits.\n2. We'll also form a secondary bit array with as many bits as the number of blocks we have. In this\n   secondary array, the `k-th` bit will be on when the `k-th` block could conceivably have a most\n   significant bit.\n3. Finally, to find `MSB(x)`, we'll first use `parallel_rank` to find the index of the most\n   significant bit in the secondary routing array. This will tell us the location of the most\n   significant block. We shall then use `parallel_rank` one more time to find the index of the most\n   significant bit in the most significant block.\n\nWe implement this three step procedure below. We begin by defining a simple abstraction.\n\n```rust\n#[derive(Debug)]\nstruct FourRussiansMSB {\n    /// The secondary routing bit array\n    macro_bit_array: u8,\n\n    /// This is simply the number whose `msb` we'd like to find.\n    /// It is logically split into blocks of 8 bits\n    micro_arrays: u64,\n}\n```\n\nThen, we implement procedures to build the two-level structure. Most of these methods use ideas that\nwe already discussed in previous sections.\n\n```rust\nimpl FourRussiansMSB {\n    pub fn build(query: u64) -> Self {\n        let macro_bit_array = Self::generate_macro_bit_array(query);\n        FourRussiansMSB {\n            macro_bit_array,\n            micro_arrays: query,\n        }\n    }\n\n    /// Generates the routing macro array. To do so, it\n    /// relies on the observation that a block contains a\n    /// 1 bit if it's highest bit is a 1 or if its\n    /// lower 7 bits' numeric value is greater than 0.\n    fn generate_macro_bit_array(query: u64) -> u8 {\n        // The first step is to extract information about the highest bit in each block.\n        let high_bit_mask = 0b10000000_10000000_10000000_10000000_10000000_10000000_10000000_10000000u64;\n        let is_high_bit_set = query & high_bit_mask;\n\n        // The second step is to extract information about the lower seven bits\n        // in each block. To do so, we use parallel_compare, which is basically\n        // subtraction.\n        let packed_ones = 0b00000001_00000001_00000001_00000001_00000001_00000001_00000001_00000001u64;\n        let mut are_lower_bits_set = query | high_bit_mask;\n        are_lower_bits_set -= packed_ones;\n        are_lower_bits_set &= high_bit_mask;\n\n        // We unify the information from the first two steps into a single value\n        // that tells us if a block could conceivably contain the MSB\n        let is_block_active = is_high_bit_set | are_lower_bits_set;\n\n        // To generate the macro array, we need to form an 8-bit number out of the\n        // per-block highest bits from the last step. To pack them together, we simply use\n        // an appropriate multiplier which does the work of a series of bitshifts\n        let packer = 0b10000001_00000010_00000100_00001000_00010000_00100000_010000001u64;\n        let mut macro_bit_array = is_block_active as u128 * packer as u128;\n        macro_bit_array >>= 49;\n        if is_block_active >> 56 == 0 {\n            macro_bit_array &= 0b0111_1111;\n        } else {\n            macro_bit_array |= 0b1000_0000;\n            macro_bit_array &= 0b1111_1111;\n        }\n        macro_bit_array as u8\n    }\n\n    pub fn get_msb(&self) -> u8 {\n        let block_id = self.msb_by_rank(self.macro_bit_array);\n        let block_start = (block_id - 1) * 8;\n        let msb_block = self.get_msb_block(block_start);\n        let msb = self.msb_by_rank(msb_block);\n        let in_block_location = msb - 1;\n        block_start + in_block_location\n    }\n\n    /// Given a block id -- which is the msb value in the macro routing array,\n    /// this method retrieves the 8 bits that represent that block\n    /// from the `micro_arrays`. `block_id` 0 refers to the highest\n    fn get_msb_block(&self, block_start: u8) -> u8 {\n        let block_mask = 0b1111_1111u64;\n        let mut block = self.micro_arrays >> block_start;\n        block &= block_mask;\n        block as u8\n    }\n\n    /// Finds the index of the most significant bit in the\n    /// provided 8-bit number by finding its rank among the\n    /// 8 possible powers of 2: \u003C1, 2, 4, 8, 16, 32, 64, 128>.\n    /// To do so in constant time, it employs techniques from\n    /// our discussion of `parallel_rank`\n    fn msb_by_rank(&self, query: u8) -> u8 {\n        // Perform the parallel comparison\n        let tiled_query = Self::parallel_tile_128(query);\n        let packed_keys = 0b000000001_000000010_000000100_000001000_000010000_000100000_001000000_010000000u128;\n        let mut difference = tiled_query - packed_keys;\n\n        // Isolate the spacer sentinel bits\n        let sentinel_mask = 0b100000000_100000000_100000000_100000000_100000000_100000000_100000000_100000000u128;\n        difference &= sentinel_mask;\n\n        // Count the number of spacer bits that are turned on\n        difference.count_ones() as u8\n    }\n\n    /// Produces a number that is a result of replicating the query\n    /// eight times. This uses 72 bits of space.\n    pub fn parallel_tile_128(query: u8) -> u128 {\n        let multiplier = 0b100000000_100000000_100000000_100000000_100000000_100000000_100000000_1000000001u128;\n\n        // Produce the provisional tiled number. We still need to set its\n        // sentinel bits to 1\n        let tiled_query = query as u128 * multiplier;\n\n        // The bitmask to turn on  the sentinel bits\n        let sentinel_mask = 0b100000000_100000000_100000000_100000000_100000000_100000000_100000000_100000000u128;\n\n        // Set the sentinel bits to 1 and return the tiled number\n        tiled_query | sentinel_mask\n    }\n}\n```\n\nWith the abstractions above, implementing `MSB(x)` is as simple as:\n\n```rust\npub fn get_msb_idx_of(query: u64) -> u8 {\n    FourRussiansMSB::build(query).get_msb()\n}\n```\n\n### $O(1)$ `LCP(x, y)`\n\nThe bit version of the LCP asks us to find the length of the longest common prefix between the\nbit-strings of the two numbers. This can simply be solved by leveraging our $O(1)$ `MSB(n)`\nprocedure from above.\n\n```rust\npub fn lcp_len_of(a: u64, b: u64) -> u64 {\n    63 - get_msb_idx_of(a ^ b) as u64\n}\n```\n\n[Here's a link to all the functions we've implemented.](https://play.rust-lang.org/?version=stable&mode=debug&edition=2018&gist=4c4975e8474c28d53c35abbe2feb1977)\n\n## References\n\n1. [CS 166 Lecture 15](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/15/Slides15.pdf)\n2. [CS 166 Lecture 16](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/16/Slides16.pdf)\n3. [CS 166 Lecture 17](http://web.stanford.edu/class/archive/cs/cs166/cs166.1196/lectures/17/Slides17.pdf)\n4. [6.851](http://courses.csail.mit.edu/6.851/fall17/scribe/lec12.pdf)\n5. [The Original Fusion Tree Paper](https://reader.elsevier.com/reader/sd/pii/0022000093900404?token=1610EF62181DAC974715067B85459A4709A9BC64E39827CE0369C6C8E18540DFD1DBAD38BEE35BFF95C4C05E45A1D1D5)\n6. [This StackOverflow Question — scroll down to the answer by `templatetypedef`](https://stackoverflow.com/questions/3878320/understanding-fusion-trees)","src/content/writing/wlp.mdx","4ad725412dfbcc20","wlp.mdx","sketches",["Map",171,172,184,185,194,195,204,205,216,217,226,227,235,236,247,248,257,258,269,270,281,282,290,291,300,301],"birds-and-frogs",{"id":171,"data":173,"body":180,"filePath":181,"digest":182,"legacyId":183,"deferredRender":17},{"title":174,"date":175,"type":176,"attribution":177,"attributionUrl":178,"draft":60,"refs":179},"On Maths, Birds, and Frogs",["Date","2023-08-13T00:00:00.000Z"],"snippet","Tai-Danae Bradley","https://math3ma.institute/wp-content/uploads/2022/02/bradley_spring22.pdf",[],"Rephrasing ideas in the precise language of mathematics allows questions to be asked and\nanswered in more useful, quantitative ways. ... A bird's eye view of the landscape is a\nvaluable perspective. It allows the observer to discover unexpected connections between\ndifferent parts. However, in order to make those connections precise and rigorous, one\nrequires the frog's attention to detail. One need to, as the frog does, frolic in the mud.","src/content/sketches/birds-and-frogs.mdx","f123e57ec3ea42c3","birds-and-frogs.mdx","audible-rhyme",{"id":184,"data":186,"body":190,"filePath":191,"digest":192,"legacyId":193,"deferredRender":17},{"title":187,"date":188,"type":67,"draft":60,"refs":189},"On The Audible Rhyme",["Date","2023-08-22T00:00:00.000Z"],[],"There's a sense in which the process by which Neural Networks are trained is similar to the\nprocess through which complex biological black boxes (organisms) are generated. More specifically,\nthere is an audible rhyme between backpropagation-mediated model training via error minimization\nand organismal evolution via natural selection. Evidently, both processes produce systems that\ndemonstrate remarkable, useful traits. Further, both processes involve the search through some\nabstract fitness landscape in order to find 'fit' models/organisms.\n\nSetting aside the philosophical question of whether deep learning models are like organisms (they\nclearly aren't), what can we gain if we treat them as though they were? What empirical questions\ncan we ask of them? Within Biology, we have developed a rich set of experimental ideas, methods,\nand tools in our quest to understand and interpret biological black boxes. Is it possible to use\nthese ideas, tools, and techniques to develop a general framework for understanding and interpreting\ntrained Neural Networks?\n\nConcretely, can we draw inspiration from Molecular Biology to design experiments geared towards\nanswering the following questions?\n\n- Do the weights of neural networks — the networks' learned representations — contain functional modules?\n- Are these functional modules, if they exist, associated in some fashion to the traits exhibited by the trained networks?\n- Can we isolate these functional modules, if they exist, and confirm their association to particular traits through Gain or Loss of function experiments?\n\nSee also: [Another Rough Perspective](https://colah.github.io/notes/bio-analogies/) ·\n[Preliminary Investigations](https://github.com/jlikhuva/blog/blob/main/slides/Genetics+NN.pdf)","src/content/sketches/audible-rhyme.mdx","1917224b9035fe7e","audible-rhyme.mdx","aristotelian-nn",{"id":194,"data":196,"body":200,"filePath":201,"digest":202,"legacyId":203,"deferredRender":17},{"title":197,"date":198,"type":67,"draft":60,"refs":199},"Interpreting Neural Networks via Aristotelian Causal Categories",["Date","2023-08-25T00:00:00.000Z"],[],"Applying Aristotle's four causes — material, formal, efficient, and final — as an interpretive\nlens for understanding what neural networks learn and how they arrive at their outputs.","src/content/sketches/aristotelian-nn.mdx","60cdc7582aa379be","aristotelian-nn.mdx","descartes-rosen",{"id":204,"data":206,"body":212,"filePath":213,"digest":214,"legacyId":215,"deferredRender":17},{"title":207,"date":208,"type":176,"attribution":209,"attributionUrl":210,"draft":60,"refs":211},"Descartes + Rosen",["Date","2020-06-01T00:00:00.000Z"],"Descartes — Rules for the Direction of the Mind","http://dt.pepperdine.edu/descartes-rules-for-direction-of-the-mind.html",[],"Just as proteins need to fold into a 3D conformation in order to become active, so too do\nideas. One should fold ideas in order to bring them into close contact. Great insights emerge\nfrom this process.\n\nWhen we have intuitively understood some simple propositions ... it is useful to go through\nthem with a continuous, uninterrupted motion of thought, to meditate upon their mutual\nrelations, and to conceive, distinctly, several of them — as many as possible — simultaneously.\nIn this manner, our knowledge will grow more certain, and the capacity of the mind will\nnotably increase.","src/content/sketches/descartes-rosen.mdx","c72554a907d1de57","descartes-rosen.mdx","find-lose-move",{"id":216,"data":218,"body":222,"filePath":223,"digest":224,"legacyId":225,"deferredRender":17},{"title":219,"date":220,"type":67,"draft":60,"refs":221},"Find it; Lose it; Move it",["Date","2022-06-01T00:00:00.000Z"],[],"A framework for understanding biological development and morphogenesis, inspired by\n[Scott Gilbert](https://vimeo.com/136532431).","src/content/sketches/find-lose-move.mdx","620ded4027e6e3cf","find-lose-move.mdx","llm-hallucinations-gibbs",{"id":226,"data":228,"filePath":232,"digest":233,"legacyId":234,"deferredRender":17},{"title":229,"date":230,"type":67,"draft":60,"refs":231},"LLM Hallucinations are manifestations of Gibbs Phenomena",["Date","2025-06-01T00:00:00.000Z"],[],"src/content/sketches/llm-hallucinations-gibbs.mdx","d63a5eaf79d23cdc","llm-hallucinations-gibbs.mdx","polya-plausible-reasoning",{"id":235,"data":237,"body":243,"filePath":244,"digest":245,"legacyId":246,"deferredRender":17},{"title":238,"date":239,"type":176,"attribution":240,"attributionUrl":241,"draft":60,"refs":242},"Polya: On Plausible Reasoning in Mathematics",["Date","2020-06-01T00:00:00.000Z"],"George Polya","https://press.princeton.edu/books/paperback/9780691025094/mathematics-and-plausible-reasoning-volume-1",[],"Mathematics is regarded as a demonstrative science. Yet, this is only one of its aspects.\nFinished mathematics presented in a finished form appears as purely demonstrative, consisting\nof proofs only. Yet, mathematics in the making resembles any other human knowledge in the\nmaking. You have to guess a mathematical theorem before you can prove it [that is, you have\nto come up with a conjecture]. You have to guess the idea of the proof before you carry\nthrough the details; You have to combine observations and follow analogies; ... the result\nof the mathematician's creative work is demonstrative reasoning; but, the proof is discovered\nby plausible reasoning, by guessing.","src/content/sketches/polya-plausible-reasoning.mdx","d18bfa710d5bdeb0","polya-plausible-reasoning.mdx","nn-graph-theory",{"id":247,"data":249,"body":253,"filePath":254,"digest":255,"legacyId":256,"deferredRender":17},{"title":250,"date":251,"type":67,"draft":60,"refs":252},"Interpreting Neural Networks using Graph Theory",["Date","2023-08-09T00:00:00.000Z"],[],"Neural Networks comprise an ordered list of matrices with interspersed non-linear functions.\nEvery matrix is, in general, [equivalent to a weighted bipartite graph](https://www.math3ma.com/blog/matrices-probability-graphs).\nTherefore, a neural network can be viewed as an ordered list of weighted bipartite graphs.\nCan we use this correspondence to translate the tools developed in Graph Theory to aid our\nunderstanding of Neural Networks? What does Ford-Fulkerson have to say, if anything, about\nhow information flows through a neural network?","src/content/sketches/nn-graph-theory.mdx","caa0bd7c7743eff3","nn-graph-theory.mdx","piketty-opener",{"id":257,"data":259,"body":265,"filePath":266,"digest":267,"legacyId":268,"deferredRender":17},{"title":260,"date":261,"type":176,"attribution":262,"attributionUrl":263,"draft":60,"refs":264},"Piketty's Powerful Opener",["Date","2020-06-01T00:00:00.000Z"],"Thomas Piketty — Capital and Ideology","https://www.hup.harvard.edu/catalog.php?isbn=9780674980822",[],"Every human society must justify its inequalities: unless reasons for them are found, the\nwhole political and social edifice stands in danger of collapse. Every epoch, therefore,\ndevelops a range of contradictory discourses and ideologies for the purpose of legitimizing\nthe inequality that already exists or the people believe should exist. ... In today's\nsocieties, these justificatory narratives comprise themes of property, entrepreneurship,\nand meritocracy ...","src/content/sketches/piketty-opener.mdx","0192aaa739c617c9","piketty-opener.mdx","rosen-basements",{"id":269,"data":271,"body":277,"filePath":278,"digest":279,"legacyId":280,"deferredRender":17},{"title":272,"date":273,"type":176,"attribution":274,"attributionUrl":275,"draft":60,"refs":276},"Rosen: Basements",["Date","2023-09-01T00:00:00.000Z"],"Robert Rosen","https://amzn.to/3ETMcRC",[],"I feel it necessary to apologize in advance for what I'm now going to discuss. No one likes\nto come down from the top of a tall building, from where vistas and panoramas are visible,\nand inspect a windowless basement. We know, intellectually, that there could be no panoramas\nwithout the basement, but emotionally, we feel no desire to look at it directly; indeed, we\nfeel an aversion. Above all, there's no beauty; there are only dark corners and dampness and\nairlessness. It is sufficient to know that the building stands on it, that its supports, its\npipes, and plumbing are in place and functioning.","src/content/sketches/rosen-basements.mdx","7b24e8cd64cc26b5","rosen-basements.mdx","rosen-taylor",{"id":281,"data":283,"filePath":287,"digest":288,"legacyId":289,"deferredRender":17},{"title":284,"date":285,"type":176,"attribution":274,"attributionUrl":275,"draft":60,"refs":286},"Rosen: Taylor's Theorem, Recursion, and Time Travel",["Date","2023-09-01T00:00:00.000Z"],[],"src/content/sketches/rosen-taylor.mdx","15680a8dfa0dfd6a","rosen-taylor.mdx","theorems-bfs",{"id":290,"data":292,"body":296,"filePath":297,"digest":298,"legacyId":299,"deferredRender":17},{"title":293,"date":294,"type":67,"draft":60,"refs":295},"Proving Theorems is an Exercise in Breadth First Search",["Date","2023-08-11T00:00:00.000Z"],[],"Mathematical ideas (definitions, theorems ...) can be thought of as nodes in a directed graph\ndepicting the entailment relationships among them. When equipped with such an entailment graph,\nproving a theorem becomes an exercise in searching for a path between some two particular nodes.\nTo prove a theorem, we start by exploring nodes in the neighborhood of our proposition. We\nsystematically expand our search until we find the node corresponding to our conclusion. Since\nmost theorems would include multiple nodes as starting points (i.e. multiple premises), we could\nalso think in terms of paths from meta-nodes to nodes.\n\nWith this framework, we can note the following: First, different proofs to the same theorem\ncorrespond to different paths between the same pair of nodes. Secondly, learning mathematics\nis the act of building a mental version of this entailment graph.\n\nThis general idea can be extended beyond logical reasoning to encompass\n[plausible reasoning](https://en.wikipedia.org/wiki/Plausible_reasoning) by equipping each\nedge with a non-negative weight representing the degree to which we believe that the source\nentails the destination.","src/content/sketches/theorems-bfs.mdx","b865ccd6ee53bea7","theorems-bfs.mdx","what-happens-if",{"id":300,"data":302,"body":306,"filePath":307,"digest":308,"legacyId":309,"deferredRender":17},{"title":303,"date":304,"type":67,"draft":60,"refs":305},"What Happens If ...",["Date","2023-08-25T00:00:00.000Z"],[],"... we row reduce the matrices of a neural network after training? If we replace the matrices\nof the network with their row-reduced counterparts, does the network retain its performance?\nDo we need to change the basis of the inputs to each layer in any fashion?\n\nRecall that row reduction is an invertible operation that corresponds to left-multiplying a\nmatrix $M$ with elementary matrices: $\\widetilde{M} = E_k \\ldots E_1 M$.","src/content/sketches/what-happens-if.mdx","35899c9d2bb27aef","what-happens-if.mdx"]