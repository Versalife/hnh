---
title: "On The Audible Rhyme"
date: 2023-08-22
type: sketch
---

There's a sense in which the process by which Neural Networks are trained is similar to the
process through which complex biological black boxes (organisms) are generated. More specifically,
there is an audible rhyme between backpropagation-mediated model training via error minimization
and organismal evolution via natural selection. Evidently, both processes produce systems that
demonstrate remarkable, useful traits. Further, both processes involve the search through some
abstract fitness landscape in order to find 'fit' models/organisms.

Setting aside the philosophical question of whether deep learning models are like organisms (they
clearly aren't), what can we gain if we treat them as though they were? What empirical questions
can we ask of them? Within Biology, we have developed a rich set of experimental ideas, methods,
and tools in our quest to understand and interpret biological black boxes. Is it possible to use
these ideas, tools, and techniques to develop a general framework for understanding and interpreting
trained Neural Networks?

Concretely, can we draw inspiration from Molecular Biology to design experiments geared towards
answering the following questions?

- Do the weights of neural networks — the networks' learned representations — contain functional modules?
- Are these functional modules, if they exist, associated in some fashion to the traits exhibited by the trained networks?
- Can we isolate these functional modules, if they exist, and confirm their association to particular traits through Gain or Loss of function experiments?

See also: [Another Rough Perspective](https://colah.github.io/notes/bio-analogies/) ·
[Preliminary Investigations](https://github.com/jlikhuva/blog/blob/main/slides/Genetics+NN.pdf)
